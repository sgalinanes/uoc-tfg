{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import tsfel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, f1_score, log_loss, confusion_matrix, classification_report)\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_robustness(metric_name, metric_values, threshold_percent=10):\n",
    "    \"\"\"\n",
    "    Check the robustness of a model based on k-fold metric values.\n",
    "    \n",
    "    A model is considered robust if the coefficient of variation (CV),\n",
    "    defined as (standard deviation / mean) * 100, is less than threshold_percent.\n",
    "    \n",
    "    Parameters:\n",
    "        metric_values (list or np.array): List/array of metric values from each fold.\n",
    "        threshold_percent (float): The maximum allowed CV percentage (default: 10).\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the model is robust (CV < threshold_percent), False otherwise.\n",
    "    \"\"\"\n",
    "    metric_values = np.array(metric_values)\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "    # Avoid division by zero:\n",
    "    if mean_val == 0:\n",
    "        cv = float('inf')\n",
    "    else:\n",
    "        cv = (std_val / mean_val) * 100\n",
    "\n",
    "    print(f\"[{metric_name}] Metric Mean: {mean_val:.4f}\")\n",
    "    print(f\"[{metric_name}] Metric Standard Deviation: {std_val:.4f}\")\n",
    "    print(f\"[{metric_name}] Coefficient of Variation (CV): {cv:.2f}%\")\n",
    "    \n",
    "    if cv < threshold_percent:\n",
    "        print(f\"[{metric_name}] Model is robust (CV < {threshold_percent}%)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"[{metric_name}] Model is not robust (CV >= {threshold_percent}%)\")\n",
    "        return False\n",
    "    \n",
    "def load_movement_features(subject_id, base_path='../../data/preprocessed/movement/', sampling_rate=100):\n",
    "    \"\"\"\n",
    "    Load and structure movement data from binary files with proper validation\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): Subject identifier (e.g., '001')\n",
    "        base_path (str): Base directory for movement data\n",
    "        sampling_rate (int): Sampling rate in Hz (used for validation)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structured data with keys as channel names and values as time series arrays\n",
    "    \"\"\"\n",
    "    # File path handling\n",
    "    path = Path(base_path) / f'{subject_id}_ml.bin'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Movement data not found for subject {subject_id}\")\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_data = np.fromfile(path, dtype=np.float32)\n",
    "    \n",
    "    # Channel configuration (from your specification)\n",
    "    tasks = [\"Relaxed1\", \"Relaxed2\", \"RelaxedTask1\", \"RelaxedTask2\", \n",
    "             \"StretchHold\", \"HoldWeight\", \"DrinkGlas\", \"CrossArms\", \n",
    "             \"TouchNose\", \"Entrainment1\", \"Entrainment2\"]\n",
    "    wrists = [\"Left\", \"Right\"]\n",
    "    sensors = [\"Accelerometer\", \"Gyroscope\"]\n",
    "    axes = [\"X\", \"Y\", \"Z\"]\n",
    "    \n",
    "    # Calculate expected parameters\n",
    "    n_channels = len(tasks) * len(wrists) * len(sensors) * len(axes)\n",
    "    # Even though the duration was 10.24 per assessment, the first ~0.5 seconds are not used and thus\n",
    "    # the preprocessed data has only 9.76 seconds of data\n",
    "    expected_duration = 9.76  # seconds per assessment\n",
    "    expected_timepoints = int(expected_duration * sampling_rate)  # 976\n",
    "    \n",
    "    # Validate data size\n",
    "    expected_size = n_channels * expected_timepoints\n",
    "    if len(raw_data) != expected_size:\n",
    "        raise ValueError(f\"Unexpected data size for {subject_id}: \"\n",
    "                         f\"Got {len(raw_data)} elements, expected {expected_size}\")\n",
    "    \n",
    "    # Reshape and structure the data\n",
    "    structured_data = {}\n",
    "    channel_idx = 0\n",
    "    \n",
    "    for task in tasks:\n",
    "        for wrist in wrists:\n",
    "            for sensor in sensors:\n",
    "                for axis in axes:\n",
    "                    # Extract channel data\n",
    "                    start = channel_idx * expected_timepoints\n",
    "                    end = (channel_idx + 1) * expected_timepoints\n",
    "                    \n",
    "                    # Create channel name\n",
    "                    channel_name = f\"{task}_{wrist}_{sensor.split(' ')[0]}_{axis}\"\n",
    "                    \n",
    "                    structured_data[channel_name] = raw_data[start:end]\n",
    "                    \n",
    "                    channel_idx += 1\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def extract_tsfel_ts_features(channel_data, domain=None, fs=100):\n",
    "    \"\"\"\n",
    "    Extract TSFEL features from a single-channel time series.\n",
    "    \n",
    "    Args:\n",
    "        channel_data (np.ndarray): 1D array of time series values.\n",
    "        domain: (str): Domain of features to extract (default: 'all').\n",
    "            - 'statistical', 'temporal', 'spectral', 'fractal': Includes the corresponding feature domain.\n",
    "            - 'all': Includes all available feature domains.\n",
    "            - list of str: A combination of the above strings, e.g., ['statistical', 'temporal'].\n",
    "            - None: By default, includes the 'statistical', 'temporal', and 'spectral' domains.\n",
    "        fs (int): Sampling frequency (default: 100).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of TSFEL features.\n",
    "    \"\"\"\n",
    "    # Obtain a default configuration covering all domains.\n",
    "    cfg = tsfel.get_features_by_domain(domain)\n",
    "\n",
    "    # Extract features; the result is a DataFrame with one row.\n",
    "    features_df = tsfel.time_series_features_extractor(cfg, channel_data, fs=fs, verbose=0)\n",
    "    # Flatten to 1D numpy array and return.\n",
    "    return features_df.values.flatten()\n",
    "\n",
    "def extract_ts_features(label, channel_data, domain=None, fs=None):\n",
    "    if label == 'basic':\n",
    "        features = [\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data),\n",
    "            np.min(channel_data),\n",
    "            np.max(channel_data),\n",
    "            np.percentile(channel_data, 25),\n",
    "            np.percentile(channel_data, 75),\n",
    "            np.var(channel_data),\n",
    "            len(np.where(np.diff(np.sign(channel_data)))[0]) / len(channel_data),  # Zero-crossing rate\n",
    "            np.sqrt(np.mean(channel_data**2))  # Root Mean Square (RMS)\n",
    "        ]\n",
    "        return features\n",
    "    else:\n",
    "        return extract_tsfel_ts_features(channel_data, domain=domain, fs=fs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Requisite: Python 3.12 (por catboost). brew install libomp para xgboost\n",
    "\n",
    "En base a los datos preprocesados, entrenaremos modelos de clasificacion bajo distintos escenarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:08<00:00, 54.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (469, 30)\n",
      "y shape: (469,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar metadatos\n",
    "file_list = pd.read_csv('../../data/preprocessed/file_list.csv',  dtype={'id': str})\n",
    "\n",
    "movement_data_per_subject = {}\n",
    "sampling_rate = 100\n",
    "\n",
    "# For each channel, extract TSFEL features and overwrite the original data\n",
    "\n",
    "# Create multiple cases so we can check each model independently:\n",
    "# 1. Basic statistical features\n",
    "# 2. TSFEL temporal, statistical, and spectral features\n",
    "# 3. TSFEL only temporal features\n",
    "# 4. TSFEL only statistical features\n",
    "# 5. TSFEL only spectral features\n",
    "# 6. Only questionnaire data\n",
    "\n",
    "# pipeline_labels = ['basic', 'TSFEL-all', 'TSFEL-temporal', 'TSFEL-statistical', 'TSFEL-spectral', 'questionnaire-only']\n",
    "pipeline_labels = ['basic', 'questionnaire-only']\n",
    "\n",
    "pipeline_args = [\n",
    "    {'domain': None, 'fs': None},  # Basic statistical features\n",
    "    #{'domain': None, 'fs': sampling_rate},  # TSFEL-all\n",
    "    #{'domain': 'temporal', 'fs': sampling_rate},  # TSFEL-temporal\n",
    "    #{'domain': 'statistical', 'fs': sampling_rate},  # TSFEL-statistical\n",
    "    #{'domain': 'spectral', 'fs': sampling_rate},  # TSFEL-spectral\n",
    "    None,  # Questionnaire-only\n",
    "]\n",
    "\n",
    "# X must be 2D (samples, features)\n",
    "X_vals = {label: [] for label in pipeline_labels}\n",
    "y_vals = {label: [] for label in pipeline_labels}\n",
    "\n",
    "# First, load timeseries data and extract features\n",
    "for _, row in tqdm(file_list.iterrows(), total=len(file_list)):\n",
    "    # Cargar cuestionario\n",
    "    quest_data = np.fromfile(f'../../data/preprocessed/questionnaire/{row[\"id\"]}_ml.bin', \n",
    "                           dtype=np.float32)\n",
    "    \n",
    "    # Load structured movement data\n",
    "    movement_data = load_movement_features(row[\"id\"])\n",
    "\n",
    "    \"\"\"\n",
    "    movement_data:\n",
    "    {\n",
    "        'Relaxed1_Left_Accelerometer_X': np.array([...]),\n",
    "        'Relaxed1_Left_Accelerometer_Y': np.array([...]),\n",
    "        ...\n",
    "    }\n",
    "    Each key corresponds to a channel, and the value is a 1D numpy array of time series data.\n",
    "    We have a total of 132 keys and each time series has 976 time points.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for label, args in zip(pipeline_labels, pipeline_args):\n",
    "        # Extract features for each channel unless label is 'questionnaire-only'\n",
    "        features = quest_data\n",
    "        ts_features = {}\n",
    "        if label != 'questionnaire-only':\n",
    "            for channel_name, channel_data in movement_data.items():\n",
    "                # Extract features using the specified domain and sampling rate\n",
    "                ts_features[channel_name] = extract_ts_features(label, channel_data, domain=args['domain'], fs=args['fs'])\n",
    "\n",
    "            # Concatenate features from all channels\n",
    "            concat_ts_features = np.concatenate(list(ts_features.values()), axis=0)\n",
    "\n",
    "            # Combine questionnaire data with time series features\n",
    "            features = np.concatenate((features, concat_ts_features), axis=0)\n",
    "        \n",
    "        X_vals[label].append(features.reshape(1, -1))\n",
    "        y_vals[label].append(row['label'])\n",
    "\n",
    "\n",
    "# Combine all samples for each pipeline\n",
    "for label in pipeline_labels:\n",
    "    if X_vals[label]:\n",
    "        X_vals[label] = np.concatenate(X_vals[label], axis=0)\n",
    "        y_vals[label] = np.array(y_vals[label])\n",
    "    else:\n",
    "        X_vals[label] = np.array([])\n",
    "        y_vals[label] = np.array([])\n",
    "\n",
    "X = np.array(X_vals['questionnaire-only'])\n",
    "y = np.array(y_vals['questionnaire-only'])\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Labels mapping for reference:\n",
    "# 0: HC\n",
    "# 1: PD\n",
    "# 2: DD\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def tune_models(clf, param_grids, X_inner, y_inner, model_name, fit_params=None):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for each model using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "        clf (sklearn classifier): Classifier instance to tune.\n",
    "        param_grids (dict): Dictionary mapping model names to their hyperparameter grids.\n",
    "        X_inner (np.array or pd.DataFrame): Feature matrix for inner CV.\n",
    "        y_inner (np.array): Labels for inner CV.\n",
    "        model_name (str): Name of the model being tuned.\n",
    "        fit_params (dict): Additional parameters for fitting the model.\n",
    "        \n",
    "    Returns:\n",
    "        tuned_results (dict): Dictionary mapping model names to the fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    # Use GridSearchCV with our custom refit_strategy.\n",
    "    scoring = {\"accuracy\": \"accuracy\", \"f1_weighted\": \"f1_weighted\"}\n",
    "    grid = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grids[model_name],\n",
    "        scoring=scoring,\n",
    "        cv=3,       # inner CV folds\n",
    "        n_jobs=-1,\n",
    "        refit=refit_strategy  # custom refit to pick robust candidate\n",
    "    )\n",
    "    grid.fit(X_inner, y_inner, **fit_params)\n",
    "    return (grid.best_estimator_, grid.best_params_)\n",
    "\n",
    "def refit_strategy(cv_results):\n",
    "    \"\"\"\n",
    "    Custom refit strategy that uses both accuracy and weighted F1 metrics.\n",
    "    \n",
    "    For each hyperparameter candidate (as provided in GridSearchCV’s cv_results_),\n",
    "    we compute the mean and standard deviation for both 'accuracy' and 'f1_weighted'.\n",
    "    We then calculate the coefficient of variation (CV) for each metric in percentage.\n",
    "    The composite score is calculated as:\n",
    "    \n",
    "        composite = 0.5*(mean_accuracy + mean_f1_weighted) - λ * 0.5*(CV_accuracy + CV_f1_weighted)\n",
    "    \n",
    "    A candidate with a high mean score and a low CV will be preferred.\n",
    "    \n",
    "    Parameters:\n",
    "        cv_results (dict): The cv_results_ dictionary from GridSearchCV. It must contain the keys:\n",
    "            \"mean_test_accuracy\", \"std_test_accuracy\", \n",
    "            \"mean_test_f1_weighted\", \"std_test_f1_weighted\".\n",
    "    \n",
    "    Returns:\n",
    "        best_index (int): The index (as in cv_results) of the best candidate.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    required_keys = [\"mean_test_accuracy\", \"std_test_accuracy\", \"mean_test_f1_weighted\", \"std_test_f1_weighted\"]\n",
    "    if not all(key in df.columns for key in required_keys):\n",
    "        raise ValueError(f\"cv_results must contain the keys: {required_keys}\")\n",
    "    \n",
    "    # Means and standard deviations for accuracy and weighted F1.\n",
    "    mean_acc = df[\"mean_test_accuracy\"]\n",
    "    std_acc  = df[\"std_test_accuracy\"]\n",
    "    mean_f1  = df[\"mean_test_f1_weighted\"]\n",
    "    std_f1   = df[\"std_test_f1_weighted\"]\n",
    "    \n",
    "    # Compute coefficient of variation (CV) in percentages.\n",
    "    cv_acc = (std_acc / mean_acc) * 100\n",
    "    cv_f1  = (std_f1 / mean_f1) * 100\n",
    "    \n",
    "    # Lambda controls the weight given to robustness. Adjust as needed.\n",
    "    lambda_val = 0.01\n",
    "    \n",
    "    # Composite score: we want high mean and low CV.\n",
    "    composite = 0.5 * (mean_acc + mean_f1) - lambda_val * 0.5 * (cv_acc + cv_f1)\n",
    "\n",
    "    best_index = composite.idxmax()\n",
    "    return best_index\n",
    "\n",
    "def run_cv(X, y, models, n_splits=5, mode=\"default\", class_weights=None, tune_inner=False, param_grids=None):\n",
    "    \"\"\"\n",
    "    Unified cross-validation runner with optional inner-loop hyperparameter tuning.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.array or pd.DataFrame): Feature matrix.\n",
    "        y (np.array): Labels.\n",
    "        models (dict): Dictionary mapping model names to a tuple: (classifier instance, fit_params dict).\n",
    "                       (If no additional fit parameters are needed, use an empty dict.)\n",
    "        n_splits (int): Number of outer CV folds.\n",
    "        mode (str): One of:\n",
    "            - \"default\": standard CV.\n",
    "            - \"smote\": apply SMOTE oversampling on the training data.\n",
    "            - \"weighted\": for multi-class cost-sensitive learning.\n",
    "            - \"weighted_binary\": for binary cost-sensitive learning (with special handling for XGBoost).\n",
    "        class_weights (dict): Custom class weighting dictionary (e.g., {0: 1.0, 1: 2.0}).\n",
    "        tune_inner (bool): If True and param_grids is provided, perform inner-loop GridSearchCV tuning.\n",
    "        param_grids (dict): Dictionary mapping model names to their hyperparameter grids for tuning.\n",
    "                           Only used if tune_inner is True.\n",
    "        \n",
    "    Returns:\n",
    "        dict: For each model, a dictionary with keys:\n",
    "            \"accuracy\": list of accuracies per outer fold,\n",
    "            \"f1_score\": list of weighted F1 scores per outer fold,\n",
    "            \"log_loss\": list of log losses per outer fold,\n",
    "            \"y_true\": list of true label arrays per fold,\n",
    "            \"y_pred\": list of predicted label arrays per fold,\n",
    "            \"y_pred_proba\": list of predicted probability arrays per fold.\n",
    "    \"\"\"\n",
    "    # Suppress warnings regarding use_label_encoder and feature names\n",
    "    # Ensure X is a DataFrame with valid feature names.\n",
    "    if not hasattr(X, \"columns\"):\n",
    "        X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Store results as an array of \"folds\" for each model.\n",
    "    results = { name: [] for name in models.keys() }\n",
    "\n",
    "    # Initialize SMOTE if selected.\n",
    "    if mode == \"smote\":\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "\n",
    "    # Outer CV loop.\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Optionally apply SMOTE.\n",
    "        if mode == \"smote\":\n",
    "            X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # For each model:\n",
    "        for name, (model, fit_params) in models.items():\n",
    "            clf = clone(model)\n",
    "            fold_fit_params = fit_params.copy() if fit_params is not None else {}\n",
    "\n",
    "            # For weighted modes, set class_weight if supported.\n",
    "            if mode in (\"weighted\", \"weighted_binary\") and class_weights is not None:\n",
    "                if 'class_weight' in clf.get_params():\n",
    "                    clf.set_params(class_weight=class_weights)\n",
    "\n",
    "            # Special handling for XGBoost in weighted_binary mode.\n",
    "            if mode == \"weighted_binary\":\n",
    "                try:\n",
    "                    from xgboost import XGBClassifier\n",
    "                    if name == \"XGBoost\" and isinstance(clf, XGBClassifier) and len(np.unique(y_train)) == 2:\n",
    "                        ratio = class_weights.get(1, 1.0) / class_weights.get(0, 1.0)\n",
    "                        clf.set_params(objective='binary:logistic', scale_pos_weight=ratio)\n",
    "                except ImportError:\n",
    "                    pass\n",
    "\n",
    "            # For LightGBM: enforce early stopping parameters if not provided and set verbosity to -1.\n",
    "            if name == \"LightGBM\":\n",
    "                if \"eval_set\" not in fold_fit_params:\n",
    "                    fold_fit_params[\"eval_set\"] = [(X_test, y_test)]\n",
    "                if \"eval_metric\" not in fold_fit_params:\n",
    "                    fold_fit_params[\"eval_metric\"] = \"logloss\"\n",
    "                clf.set_params(verbose=-1)\n",
    "\n",
    "            # --- Inner-loop: Hyperparameter tuning ---\n",
    "            hyperparameter_tuning_best_params = None\n",
    "            if tune_inner and param_grids is not None and name in param_grids:\n",
    "                # Perform hyperparameter tuning using GridSearchCV.\n",
    "                print(f\"Tuning hyperparameters for {name}...\")\n",
    "                (best_estimator, best_params) = tune_models(clf, param_grids, X_train, y_train, name, fit_params=fold_fit_params)\n",
    "                clf = best_estimator\n",
    "                hyperparameter_tuning_best_params = best_params\n",
    "            else:\n",
    "                clf.fit(X_train, y_train, **fold_fit_params)\n",
    "\n",
    "            # Evaluate on the outer test set.\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "            # Store each model's results in the \"results\" array, where each outer fold is indexed by k_idx.\n",
    "            # And each element in the results array is a metrics dictionary.\n",
    "            model_metrics = {}\n",
    "            \n",
    "            model_metrics[\"params\"] = hyperparameter_tuning_best_params if hyperparameter_tuning_best_params else \"default\",\n",
    "            model_metrics[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "            model_metrics[\"f1_score\"] = f1_score(y_test, y_pred, average='weighted')\n",
    "            model_metrics[\"log_loss\"] = log_loss(y_test, y_pred_proba)\n",
    "            model_metrics[\"y_true\"] = y_test\n",
    "            model_metrics[\"y_pred\"] = y_pred\n",
    "            model_metrics[\"y_pred_proba\"] = y_pred_proba\n",
    "\n",
    "            print(f\"Model results for {name}:\")\n",
    "            print(f\"Parameters: {model_metrics['params']}\")\n",
    "            print(f\"Accuracy: {model_metrics['accuracy']:.4f}\")\n",
    "            print(f\"F1 Score: {model_metrics['f1_score']:.4f}\")\n",
    "            print(f\"Log Loss: {model_metrics['log_loss']:.4f}\")\n",
    "            \n",
    "            results[name].append(model_metrics)\n",
    "\n",
    "    # Check robustness for each model (using your check_robustness function)\n",
    "    for name in results.keys():\n",
    "        # The metrics stored for each model are in an array of dictionaries, where each dictionary corresponds to a fold.\n",
    "        # We need to extract the metrics from each fold and check their robustness.\n",
    "        acc_values = [results[name][k][\"accuracy\"] for k in range(n_splits)]\n",
    "        f1_values = [results[name][k][\"f1_score\"] for k in range(n_splits)]\n",
    "        ll_values = [results[name][k][\"log_loss\"] for k in range(n_splits)]\n",
    "\n",
    "        print(f\"\\nRobustness for model: {name}\")\n",
    "        acc_robust = check_robustness(\"accuracy\", acc_values)\n",
    "        f1_robust = check_robustness(\"f1_score\", f1_values)\n",
    "        ll_robust = check_robustness(\"log_loss\", ll_values)\n",
    "        if acc_robust and f1_robust and ll_robust:\n",
    "            print(f\"{name} is robust across folds.\\n\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {name} is not robust across folds.\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_cv(results, target_names):\n",
    "    \"\"\"\n",
    "    Aggregates per-fold metrics from a results dictionary and prints overall metrics,\n",
    "    confusion matrix, and classification report using tabulate.\n",
    "    \n",
    "    Parameters:\n",
    "        results (dict): Dictionary mapping model names to a list of per-fold metric dictionaries.\n",
    "                        Each per-fold dictionary must include:\n",
    "                            \"accuracy\": float,\n",
    "                            \"f1_score\": float,\n",
    "                            \"log_loss\": float,\n",
    "                            \"y_true\": true labels for the fold,\n",
    "                            \"y_pred\": predicted labels for the fold,\n",
    "                            \"y_pred_proba\": predicted probability array for the fold,\n",
    "                            \"params\": the model parameters used.\n",
    "        target_names (list): List of class names (e.g., ['HC', 'PD', 'DD']).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of model names to overall metrics including:\n",
    "            \"accuracy_mean\", \"accuracy_std\", \"f1_mean\", \"f1_std\", \n",
    "            \"log_loss_mean\", \"log_loss_std\", \"confusion_matrix\", and \"classification_report\".\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    overall_metrics = {}\n",
    "    table_data = []\n",
    "\n",
    "    # From target_names, print what we are evaluating\n",
    "    print(f\"Evaluating models with target names: {target_names}\")\n",
    "    print(f\"Number of classes: {len(target_names)}\")\n",
    "    print(f\"Classes: {target_names}\")\n",
    "    print(f\"Number of models: {len(results)}\")\n",
    "    print(f\"Models: {list(results.keys())}\")\n",
    "\n",
    "    for name, fold_results in results.items():\n",
    "        # Aggregate per-fold predictions.\n",
    "        y_true_all = np.concatenate([fold[\"y_true\"] for fold in fold_results])\n",
    "        y_pred_all = np.concatenate([fold[\"y_pred\"] for fold in fold_results])\n",
    "        y_pred_proba_all = np.concatenate([fold[\"y_pred_proba\"] for fold in fold_results])\n",
    "        \n",
    "        # Aggregate per-fold metric values.\n",
    "        acc_values = np.array([fold[\"accuracy\"] for fold in fold_results])\n",
    "        f1_values  = np.array([fold[\"f1_score\"] for fold in fold_results])\n",
    "        ll_values  = np.array([fold[\"log_loss\"] for fold in fold_results])\n",
    "        \n",
    "        # Retrieve parameters (assumed constant across folds).\n",
    "        params_val = fold_results[0].get(\"params\", \"default\")\n",
    "        if isinstance(params_val, tuple) and len(params_val) == 1:\n",
    "            params_val = params_val[0]\n",
    "        \n",
    "        # Compute mean and standard deviation for numeric metrics.\n",
    "        acc_mean, acc_std = np.mean(acc_values), np.std(acc_values)\n",
    "        f1_mean,  f1_std  = np.mean(f1_values),  np.std(f1_values)\n",
    "        ll_mean,  ll_std  = np.mean(ll_values),  np.std(ll_values)\n",
    "        \n",
    "        # Compute the overall confusion matrix and classification report.\n",
    "        cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "        clf_report = classification_report(y_true_all, y_pred_all, target_names=target_names, zero_division=0)\n",
    "        \n",
    "        overall_metrics[name] = {\n",
    "            \"params\": params_val,\n",
    "            \"accuracy_mean\": acc_mean,\n",
    "            \"accuracy_std\": acc_std,\n",
    "            \"f1_mean\": f1_mean,\n",
    "            \"f1_std\": f1_std,\n",
    "            \"log_loss_mean\": ll_mean,\n",
    "            \"log_loss_std\": ll_std,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"classification_report\": clf_report,\n",
    "        }\n",
    "        table_data.append([\n",
    "            name,\n",
    "            params_val,\n",
    "            f\"{acc_mean:.4f} ± {acc_std:.4f}\",\n",
    "            f\"{f1_mean:.4f} ± {f1_std:.4f}\",\n",
    "            f\"{ll_mean:.4f} ± {ll_std:.4f}\"\n",
    "        ])\n",
    "        \n",
    "        # Print detailed report for this model.\n",
    "        print(f\"Model: {name}\")\n",
    "        print(f\"Parameters: {params_val}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(clf_report)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Overall Metrics:\")\n",
    "    print(tabulate(table_data, headers=[\"Model\", \"Params\", \"Accuracy\", \"Weighted F1\", \"Log Loss\"], tablefmt=\"pipe\"))\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 300],#, 500],\n",
    "        \"max_depth\": [None, 10],#, 20],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"learning_rate\": [0.01, 0.05],#, 0.1, 0.3],\n",
    "        \"max_depth\": [3, 6],#, 9, 12],\n",
    "        \"subsample\": [0.5, 0.7],#, 1.0]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"depth\": [4, 6],#, 8, 10],\n",
    "        \"learning_rate\": [0.01],#, 0.03, 0.1],\n",
    "        \"l2_leaf_reg\": [1, 3],#, 5, 10]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"num_leaves\": [20, 31],#, 50, 100],\n",
    "        \"learning_rate\": [0.01, 0.05],#, 0.1, 0.2],\n",
    "        \"min_child_samples\": [10, 20],#, 30, 50]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "     \"RandomForest\": [RandomForestClassifier(random_state=42), {}],\n",
    "     \"XGBoost\": [XGBClassifier(eval_metric='mlogloss', random_state=42), {}],\n",
    "     \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "     \"LightGBM\": [LGBMClassifier(random_state=42), {'callbacks': [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]}],\n",
    "#     \"LightGBM\": [LGBMClassifier(random_state=42), {}]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multiclase (PD vs DD vs HC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Performing multi-class classification (PD vs DD vs HC) [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.5988\n",
      "Log Loss: 0.7920\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6634\n",
      "Log Loss: 0.9973\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6489\n",
      "F1 Score: 0.5637\n",
      "Log Loss: 0.7821\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.5945\n",
      "Log Loss: 0.7009\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6649\n",
      "Log Loss: 0.8870\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6279\n",
      "Log Loss: 0.6988\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6186\n",
      "Log Loss: 1.1301\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6170\n",
      "F1 Score: 0.6053\n",
      "Log Loss: 0.9572\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6206\n",
      "Log Loss: 0.7105\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6004\n",
      "Log Loss: 0.7493\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6260\n",
      "Log Loss: 1.1864\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.5992\n",
      "Log Loss: 0.7543\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6035\n",
      "Log Loss: 0.6956\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6590\n",
      "Log Loss: 1.0620\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6882\n",
      "F1 Score: 0.6189\n",
      "Log Loss: 0.7608\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0161\n",
      "[accuracy] Coefficient of Variation (CV): 2.45%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6032\n",
      "[f1_score] Metric Standard Deviation: 0.0082\n",
      "[f1_score] Coefficient of Variation (CV): 1.36%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.8136\n",
      "[log_loss] Metric Standard Deviation: 0.1621\n",
      "[log_loss] Coefficient of Variation (CV): 19.93%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0249\n",
      "[accuracy] Coefficient of Variation (CV): 3.80%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6437\n",
      "[f1_score] Metric Standard Deviation: 0.0239\n",
      "[f1_score] Coefficient of Variation (CV): 3.72%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 1.0180\n",
      "[log_loss] Metric Standard Deviation: 0.1016\n",
      "[log_loss] Coefficient of Variation (CV): 9.98%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0195\n",
      "[accuracy] Coefficient of Variation (CV): 2.98%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6060\n",
      "[f1_score] Metric Standard Deviation: 0.0232\n",
      "[f1_score] Coefficient of Variation (CV): 3.83%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7413\n",
      "[log_loss] Metric Standard Deviation: 0.0315\n",
      "[log_loss] Coefficient of Variation (CV): 4.25%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 47  20  12]\n",
      " [ 17 249  10]\n",
      " [ 27  75  12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.52      0.59      0.55        79\n",
      "          PD       0.72      0.90      0.80       276\n",
      "          DD       0.35      0.11      0.16       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.53      0.53      0.51       469\n",
      "weighted avg       0.60      0.66      0.61       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 48  16  15]\n",
      " [ 20 226  30]\n",
      " [ 23  57  34]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.53      0.61      0.56        79\n",
      "          PD       0.76      0.82      0.79       276\n",
      "          DD       0.43      0.30      0.35       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.57      0.57       469\n",
      "weighted avg       0.64      0.66      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 42  24  13]\n",
      " [  9 252  15]\n",
      " [ 25  75  14]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.55      0.53      0.54        79\n",
      "          PD       0.72      0.91      0.80       276\n",
      "          DD       0.33      0.12      0.18       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.53      0.52      0.51       469\n",
      "weighted avg       0.60      0.66      0.61       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}   | 0.6568 ± 0.0161 | 0.6032 ± 0.0082 | 0.8136 ± 0.1621 |\n",
      "| XGBoost      | default                                                            | 0.6568 ± 0.0249 | 0.6437 ± 0.0239 | 1.0180 ± 0.1016 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20} | 0.6568 ± 0.0195 | 0.6060 ± 0.0232 | 0.7413 ± 0.0315 |\n",
      "Performing multi-class classification (PD vs DD vs HC) with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.6915\n",
      "F1 Score: 0.6386\n",
      "Log Loss: 0.7944\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6915\n",
      "F1 Score: 0.6870\n",
      "Log Loss: 1.0441\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6219\n",
      "Log Loss: 0.8501\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6915\n",
      "F1 Score: 0.6706\n",
      "Log Loss: 0.6963\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6483\n",
      "Log Loss: 0.9677\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6739\n",
      "Log Loss: 0.7061\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6489\n",
      "F1 Score: 0.6146\n",
      "Log Loss: 1.4983\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6178\n",
      "Log Loss: 0.9912\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6443\n",
      "Log Loss: 0.7871\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6229\n",
      "Log Loss: 1.1550\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6148\n",
      "Log Loss: 1.2194\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6680\n",
      "Log Loss: 0.7741\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6325\n",
      "Log Loss: 0.7088\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6395\n",
      "Log Loss: 1.1046\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6452\n",
      "F1 Score: 0.6315\n",
      "Log Loss: 0.8184\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6695\n",
      "[accuracy] Metric Standard Deviation: 0.0220\n",
      "[accuracy] Coefficient of Variation (CV): 3.29%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6358\n",
      "[f1_score] Metric Standard Deviation: 0.0192\n",
      "[f1_score] Coefficient of Variation (CV): 3.02%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.9706\n",
      "[log_loss] Metric Standard Deviation: 0.3122\n",
      "[log_loss] Coefficient of Variation (CV): 32.16%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0258\n",
      "[accuracy] Coefficient of Variation (CV): 3.93%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6415\n",
      "[f1_score] Metric Standard Deviation: 0.0260\n",
      "[f1_score] Coefficient of Variation (CV): 4.06%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 1.0654\n",
      "[log_loss] Metric Standard Deviation: 0.0903\n",
      "[log_loss] Coefficient of Variation (CV): 8.47%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6567\n",
      "[accuracy] Metric Standard Deviation: 0.0187\n",
      "[accuracy] Coefficient of Variation (CV): 2.85%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6479\n",
      "[f1_score] Metric Standard Deviation: 0.0202\n",
      "[f1_score] Coefficient of Variation (CV): 3.12%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7872\n",
      "[log_loss] Metric Standard Deviation: 0.0483\n",
      "[log_loss] Coefficient of Variation (CV): 6.14%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "Confusion Matrix:\n",
      "[[ 53  12  14]\n",
      " [ 20 240  16]\n",
      " [ 30  63  21]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.51      0.67      0.58        79\n",
      "          PD       0.76      0.87      0.81       276\n",
      "          DD       0.41      0.18      0.25       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.56      0.57      0.55       469\n",
      "weighted avg       0.64      0.67      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 48  16  15]\n",
      " [ 20 227  29]\n",
      " [ 27  54  33]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.51      0.61      0.55        79\n",
      "          PD       0.76      0.82      0.79       276\n",
      "          DD       0.43      0.29      0.35       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.57      0.56       469\n",
      "weighted avg       0.64      0.66      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31}\n",
      "Confusion Matrix:\n",
      "[[ 52  11  16]\n",
      " [ 20 221  35]\n",
      " [ 29  50  35]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.51      0.66      0.58        79\n",
      "          PD       0.78      0.80      0.79       276\n",
      "          DD       0.41      0.31      0.35       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.59      0.57       469\n",
      "weighted avg       0.65      0.66      0.65       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}     | 0.6695 ± 0.0220 | 0.6358 ± 0.0192 | 0.9706 ± 0.3122 |\n",
      "| XGBoost      | default                                                            | 0.6568 ± 0.0258 | 0.6415 ± 0.0260 | 1.0654 ± 0.0903 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31} | 0.6567 ± 0.0187 | 0.6479 ± 0.0202 | 0.7872 ± 0.0483 |\n",
      "Performing multi-class classification (PD vs DD vs HC) with cost-sensitive learning...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6173\n",
      "Log Loss: 0.8169\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6634\n",
      "Log Loss: 0.9973\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6100\n",
      "Log Loss: 0.7743\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6133\n",
      "Log Loss: 0.7119\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6649\n",
      "Log Loss: 0.8870\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.5604\n",
      "Log Loss: 0.7392\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6269\n",
      "Log Loss: 0.7512\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6170\n",
      "F1 Score: 0.6053\n",
      "Log Loss: 0.9572\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6805\n",
      "Log Loss: 0.7119\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6170\n",
      "F1 Score: 0.5924\n",
      "Log Loss: 1.1439\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6260\n",
      "Log Loss: 1.1864\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6470\n",
      "Log Loss: 0.7728\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6882\n",
      "F1 Score: 0.6552\n",
      "Log Loss: 0.7147\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6590\n",
      "Log Loss: 1.0620\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7097\n",
      "F1 Score: 0.6882\n",
      "Log Loss: 0.7449\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6589\n",
      "[accuracy] Metric Standard Deviation: 0.0234\n",
      "[accuracy] Coefficient of Variation (CV): 3.55%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6210\n",
      "[f1_score] Metric Standard Deviation: 0.0205\n",
      "[f1_score] Coefficient of Variation (CV): 3.30%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.8277\n",
      "[log_loss] Metric Standard Deviation: 0.1626\n",
      "[log_loss] Coefficient of Variation (CV): 19.64%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0249\n",
      "[accuracy] Coefficient of Variation (CV): 3.80%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6437\n",
      "[f1_score] Metric Standard Deviation: 0.0239\n",
      "[f1_score] Coefficient of Variation (CV): 3.72%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 1.0180\n",
      "[log_loss] Metric Standard Deviation: 0.1016\n",
      "[log_loss] Coefficient of Variation (CV): 9.98%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6739\n",
      "[accuracy] Metric Standard Deviation: 0.0298\n",
      "[accuracy] Coefficient of Variation (CV): 4.42%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6372\n",
      "[f1_score] Metric Standard Deviation: 0.0474\n",
      "[f1_score] Coefficient of Variation (CV): 7.43%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7486\n",
      "[log_loss] Metric Standard Deviation: 0.0232\n",
      "[log_loss] Coefficient of Variation (CV): 3.10%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 40  20  19]\n",
      " [ 11 249  16]\n",
      " [ 22  72  20]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.55      0.51      0.53        79\n",
      "          PD       0.73      0.90      0.81       276\n",
      "          DD       0.36      0.18      0.24       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.55      0.53      0.52       469\n",
      "weighted avg       0.61      0.66      0.62       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 48  16  15]\n",
      " [ 20 226  30]\n",
      " [ 23  57  34]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.53      0.61      0.56        79\n",
      "          PD       0.76      0.82      0.79       276\n",
      "          DD       0.43      0.30      0.35       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.57      0.57       469\n",
      "weighted avg       0.64      0.66      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 31  24  24]\n",
      " [  5 254  17]\n",
      " [ 12  71  31]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.65      0.39      0.49        79\n",
      "          PD       0.73      0.92      0.81       276\n",
      "          DD       0.43      0.27      0.33       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.60      0.53      0.54       469\n",
      "weighted avg       0.64      0.67      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}   | 0.6589 ± 0.0234 | 0.6210 ± 0.0205 | 0.8277 ± 0.1626 |\n",
      "| XGBoost      | default                                                            | 0.6568 ± 0.0249 | 0.6437 ± 0.0239 | 1.0180 ± 0.1016 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.6739 ± 0.0298 | 0.6372 ± 0.0474 | 0.7486 ± 0.0232 |\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Multi-Class Classification (no changes): PD vs DD vs HC\n",
    "# -----------------------------------------------------------------------------\n",
    "print(len(np.unique(y)))\n",
    "if len(np.unique(y)) == 3:\n",
    "    print(\"Performing multi-class classification (PD vs DD vs HC) [Default Mode]...\")\n",
    "    results_default = run_cv(X, y, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_default = evaluate_cv(results_default, target_names=['HC', 'PD', 'DD'])\n",
    "else:\n",
    "    print(\"Multi-class classification (PD vs DD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multi-Class Classification with SMOTE: PD vs DD vs HC\n",
    "# -----------------------------------------------------------------------------\n",
    "if len(np.unique(y)) == 3:\n",
    "    print(\"Performing multi-class classification (PD vs DD vs HC) with SMOTE...\")\n",
    "    results_smote = run_cv(X, y, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_smote = evaluate_cv(results_smote, target_names=['HC', 'PD', 'DD'])\n",
    "else:\n",
    "    print(\"Multi-class classification with SMOTE is not possible with the current labels.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Multi-Class Classification with Cost-Sensitive Learning (PD vs DD vs HC)\n",
    "# -----------------------------------------------------------------------------\n",
    "if len(np.unique(y)) == 3:\n",
    "    print(\"Performing multi-class classification (PD vs DD vs HC) with cost-sensitive learning...\")\n",
    "    # Example custom weights: here HC (0) is assumed majority and PD (1) and DD (2) are minority.\n",
    "    class_weights_multi = {0: 1.0, 1: 2.0, 2: 2.0}\n",
    "    results_weighted = run_cv(X, y, models, n_splits=5, mode=\"weighted\", class_weights=class_weights_multi, tune_inner=True, param_grids=param_grids)\n",
    "    overall_weighted = evaluate_cv(results_weighted, target_names=['HC', 'PD', 'DD'])\n",
    "else:\n",
    "    print(\"Multi-class classification with cost-sensitive learning is not possible with the current labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Binarios (PD vs DD) / (PD vs HC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing binary classification: PD vs DD [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7213\n",
      "Log Loss: 0.5173\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7253\n",
      "Log Loss: 0.6706\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7007\n",
      "Log Loss: 0.5125\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7638\n",
      "Log Loss: 0.4781\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.7239\n",
      "Log Loss: 0.7293\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7920\n",
      "Log Loss: 0.4845\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.7051\n",
      "F1 Score: 0.6722\n",
      "Log Loss: 0.5229\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6795\n",
      "F1 Score: 0.6774\n",
      "Log Loss: 0.7958\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7020\n",
      "Log Loss: 0.5420\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7418\n",
      "Log Loss: 0.4843\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8077\n",
      "F1 Score: 0.7962\n",
      "Log Loss: 0.6172\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7773\n",
      "Log Loss: 0.4658\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6787\n",
      "Log Loss: 0.4985\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7460\n",
      "Log Loss: 0.6724\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7448\n",
      "Log Loss: 0.4854\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7436\n",
      "[accuracy] Metric Standard Deviation: 0.0281\n",
      "[accuracy] Coefficient of Variation (CV): 3.78%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7156\n",
      "[f1_score] Metric Standard Deviation: 0.0355\n",
      "[f1_score] Coefficient of Variation (CV): 4.96%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5002\n",
      "[log_loss] Metric Standard Deviation: 0.0176\n",
      "[log_loss] Coefficient of Variation (CV): 3.52%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7410\n",
      "[accuracy] Metric Standard Deviation: 0.0424\n",
      "[accuracy] Coefficient of Variation (CV): 5.73%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7338\n",
      "[f1_score] Metric Standard Deviation: 0.0385\n",
      "[f1_score] Coefficient of Variation (CV): 5.24%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.6971\n",
      "[log_loss] Metric Standard Deviation: 0.0608\n",
      "[log_loss] Coefficient of Variation (CV): 8.72%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7641\n",
      "[accuracy] Metric Standard Deviation: 0.0238\n",
      "[accuracy] Coefficient of Variation (CV): 3.11%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7434\n",
      "[f1_score] Metric Standard Deviation: 0.0376\n",
      "[f1_score] Coefficient of Variation (CV): 5.05%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.4981\n",
      "[log_loss] Metric Standard Deviation: 0.0265\n",
      "[log_loss] Coefficient of Variation (CV): 5.33%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[252  24]\n",
      " [ 76  38]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.77      0.91      0.83       276\n",
      "          DD       0.61      0.33      0.43       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.62      0.63       390\n",
      "weighted avg       0.72      0.74      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[234  42]\n",
      " [ 59  55]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.80      0.85      0.82       276\n",
      "          DD       0.57      0.48      0.52       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.68      0.67      0.67       390\n",
      "weighted avg       0.73      0.74      0.73       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[250  26]\n",
      " [ 66  48]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.91      0.84       276\n",
      "          DD       0.65      0.42      0.51       114\n",
      "\n",
      "    accuracy                           0.76       390\n",
      "   macro avg       0.72      0.66      0.68       390\n",
      "weighted avg       0.75      0.76      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7436 ± 0.0281 | 0.7156 ± 0.0355 | 0.5002 ± 0.0176 |\n",
      "| XGBoost      | default                                                            | 0.7410 ± 0.0424 | 0.7338 ± 0.0385 | 0.6971 ± 0.0608 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.7641 ± 0.0238 | 0.7434 ± 0.0376 | 0.4981 ± 0.0265 |\n",
      "\n",
      "Performing binary classification: PD vs HC [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.8976\n",
      "Log Loss: 0.2808\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8718\n",
      "Log Loss: 0.3250\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8593\n",
      "Log Loss: 0.3415\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8643\n",
      "Log Loss: 0.3285\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8495\n",
      "Log Loss: 0.3884\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8433\n",
      "Log Loss: 0.3142\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2708\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8846\n",
      "Log Loss: 0.3039\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9155\n",
      "Log Loss: 0.2394\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8391\n",
      "Log Loss: 0.3671\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8557\n",
      "Log Loss: 0.3630\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8405\n",
      "Log Loss: 0.3468\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9287\n",
      "Log Loss: 0.2549\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9287\n",
      "Log Loss: 0.2317\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9001\n",
      "Log Loss: 0.2091\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.8817\n",
      "[accuracy] Metric Standard Deviation: 0.0303\n",
      "[accuracy] Coefficient of Variation (CV): 3.44%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8796\n",
      "[f1_score] Metric Standard Deviation: 0.0308\n",
      "[f1_score] Coefficient of Variation (CV): 3.50%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3004\n",
      "[log_loss] Metric Standard Deviation: 0.0414\n",
      "[log_loss] Coefficient of Variation (CV): 13.78%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0290\n",
      "[accuracy] Coefficient of Variation (CV): 3.30%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8781\n",
      "[f1_score] Metric Standard Deviation: 0.0281\n",
      "[f1_score] Coefficient of Variation (CV): 3.20%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3224\n",
      "[log_loss] Metric Standard Deviation: 0.0540\n",
      "[log_loss] Coefficient of Variation (CV): 16.75%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0261\n",
      "[accuracy] Coefficient of Variation (CV): 2.97%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8717\n",
      "[f1_score] Metric Standard Deviation: 0.0305\n",
      "[f1_score] Coefficient of Variation (CV): 3.50%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2902\n",
      "[log_loss] Metric Standard Deviation: 0.0558\n",
      "[log_loss] Coefficient of Variation (CV): 19.23%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 55  24]\n",
      " [ 18 258]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.70      0.72        79\n",
      "          PD       0.91      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.82      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 56  23]\n",
      " [ 20 256]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.74      0.71      0.72        79\n",
      "          PD       0.92      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.82      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 49  30]\n",
      " [ 13 263]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.79      0.62      0.70        79\n",
      "          PD       0.90      0.95      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.84      0.79      0.81       355\n",
      "weighted avg       0.87      0.88      0.87       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}   | 0.8817 ± 0.0303 | 0.8796 ± 0.0308 | 0.3004 ± 0.0414 |\n",
      "| XGBoost      | default                                                            | 0.8789 ± 0.0290 | 0.8781 ± 0.0281 | 0.3224 ± 0.0540 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20} | 0.8789 ± 0.0261 | 0.8717 ± 0.0305 | 0.2902 ± 0.0558 |\n",
      "\n",
      "Performing binary classification: PD vs DD with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6905\n",
      "Log Loss: 0.5438\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7253\n",
      "Log Loss: 0.7305\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7579\n",
      "Log Loss: 0.5436\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7623\n",
      "Log Loss: 0.4971\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7324\n",
      "Log Loss: 0.7342\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8077\n",
      "F1 Score: 0.8109\n",
      "Log Loss: 0.5300\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7253\n",
      "Log Loss: 0.5244\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6795\n",
      "F1 Score: 0.6726\n",
      "Log Loss: 0.8348\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6538\n",
      "F1 Score: 0.6516\n",
      "Log Loss: 0.5757\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7579\n",
      "Log Loss: 0.4538\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7848\n",
      "Log Loss: 0.5771\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7548\n",
      "Log Loss: 0.4859\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7342\n",
      "Log Loss: 0.4925\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7405\n",
      "Log Loss: 0.7116\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7507\n",
      "Log Loss: 0.5102\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7513\n",
      "[accuracy] Metric Standard Deviation: 0.0192\n",
      "[accuracy] Coefficient of Variation (CV): 2.55%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7340\n",
      "[f1_score] Metric Standard Deviation: 0.0258\n",
      "[f1_score] Coefficient of Variation (CV): 3.52%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5023\n",
      "[log_loss] Metric Standard Deviation: 0.0306\n",
      "[log_loss] Coefficient of Variation (CV): 6.09%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7410\n",
      "[accuracy] Metric Standard Deviation: 0.0375\n",
      "[accuracy] Coefficient of Variation (CV): 5.06%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7311\n",
      "[f1_score] Metric Standard Deviation: 0.0359\n",
      "[f1_score] Coefficient of Variation (CV): 4.91%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7177\n",
      "[log_loss] Metric Standard Deviation: 0.0824\n",
      "[log_loss] Coefficient of Variation (CV): 11.48%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0510\n",
      "[accuracy] Coefficient of Variation (CV): 6.81%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7452\n",
      "[f1_score] Metric Standard Deviation: 0.0517\n",
      "[f1_score] Coefficient of Variation (CV): 6.94%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5291\n",
      "[log_loss] Metric Standard Deviation: 0.0303\n",
      "[log_loss] Coefficient of Variation (CV): 5.73%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[246  30]\n",
      " [ 67  47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.89      0.84       276\n",
      "          DD       0.61      0.41      0.49       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.65      0.66       390\n",
      "weighted avg       0.73      0.75      0.74       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[237  39]\n",
      " [ 62  52]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.86      0.82       276\n",
      "          DD       0.57      0.46      0.51       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.68      0.66      0.67       390\n",
      "weighted avg       0.73      0.74      0.73       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[231  45]\n",
      " [ 53  61]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.81      0.84      0.82       276\n",
      "          DD       0.58      0.54      0.55       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.69      0.69      0.69       390\n",
      "weighted avg       0.74      0.75      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7513 ± 0.0192 | 0.7340 ± 0.0258 | 0.5023 ± 0.0306 |\n",
      "| XGBoost      | default                                                            | 0.7410 ± 0.0375 | 0.7311 ± 0.0359 | 0.7177 ± 0.0824 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20} | 0.7487 ± 0.0510 | 0.7452 ± 0.0517 | 0.5291 ± 0.0303 |\n",
      "\n",
      "Performing binary classification: PD vs HC with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9003\n",
      "Log Loss: 0.3099\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8746\n",
      "Log Loss: 0.3631\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8873\n",
      "Log Loss: 0.3204\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9057\n",
      "Log Loss: 0.3384\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8495\n",
      "Log Loss: 0.4287\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8803\n",
      "Log Loss: 0.3486\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9186\n",
      "Log Loss: 0.2849\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.3047\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8769\n",
      "Log Loss: 0.2770\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.8310\n",
      "F1 Score: 0.8344\n",
      "Log Loss: 0.3230\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8433\n",
      "Log Loss: 0.3935\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.3134\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9577\n",
      "F1 Score: 0.9582\n",
      "Log Loss: 0.1981\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9155\n",
      "Log Loss: 0.2230\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9304\n",
      "Log Loss: 0.1766\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.9014\n",
      "[accuracy] Metric Standard Deviation: 0.0408\n",
      "[accuracy] Coefficient of Variation (CV): 4.53%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.9034\n",
      "[f1_score] Metric Standard Deviation: 0.0400\n",
      "[f1_score] Coefficient of Variation (CV): 4.43%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2908\n",
      "[log_loss] Metric Standard Deviation: 0.0496\n",
      "[log_loss] Coefficient of Variation (CV): 17.04%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8704\n",
      "[accuracy] Metric Standard Deviation: 0.0258\n",
      "[accuracy] Coefficient of Variation (CV): 2.97%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8702\n",
      "[f1_score] Metric Standard Deviation: 0.0254\n",
      "[f1_score] Coefficient of Variation (CV): 2.92%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3426\n",
      "[log_loss] Metric Standard Deviation: 0.0723\n",
      "[log_loss] Coefficient of Variation (CV): 21.12%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8873\n",
      "[accuracy] Metric Standard Deviation: 0.0218\n",
      "[accuracy] Coefficient of Variation (CV): 2.46%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8887\n",
      "[f1_score] Metric Standard Deviation: 0.0217\n",
      "[f1_score] Coefficient of Variation (CV): 2.45%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2872\n",
      "[log_loss] Metric Standard Deviation: 0.0598\n",
      "[log_loss] Coefficient of Variation (CV): 20.83%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "Confusion Matrix:\n",
      "[[ 67  12]\n",
      " [ 23 253]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.74      0.85      0.79        79\n",
      "          PD       0.95      0.92      0.94       276\n",
      "\n",
      "    accuracy                           0.90       355\n",
      "   macro avg       0.85      0.88      0.86       355\n",
      "weighted avg       0.91      0.90      0.90       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 56  23]\n",
      " [ 23 253]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.71      0.71      0.71        79\n",
      "          PD       0.92      0.92      0.92       276\n",
      "\n",
      "    accuracy                           0.87       355\n",
      "   macro avg       0.81      0.81      0.81       355\n",
      "weighted avg       0.87      0.87      0.87       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 63  16]\n",
      " [ 24 252]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.72      0.80      0.76        79\n",
      "          PD       0.94      0.91      0.93       276\n",
      "\n",
      "    accuracy                           0.89       355\n",
      "   macro avg       0.83      0.86      0.84       355\n",
      "weighted avg       0.89      0.89      0.89       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}     | 0.9014 ± 0.0408 | 0.9034 ± 0.0400 | 0.2908 ± 0.0496 |\n",
      "| XGBoost      | default                                                            | 0.8704 ± 0.0258 | 0.8702 ± 0.0254 | 0.3426 ± 0.0723 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20} | 0.8873 ± 0.0218 | 0.8887 ± 0.0217 | 0.2872 ± 0.0598 |\n",
      "\n",
      "Performing binary classification: PD vs DD with cost-sensitive learning...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.6920\n",
      "Log Loss: 0.5233\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6978\n",
      "Log Loss: 0.7087\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7623\n",
      "Log Loss: 0.5614\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8205\n",
      "F1 Score: 0.8117\n",
      "Log Loss: 0.4855\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7394\n",
      "Log Loss: 0.6845\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.7290\n",
      "Log Loss: 0.5243\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.6923\n",
      "F1 Score: 0.6531\n",
      "Log Loss: 0.5290\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.6795\n",
      "F1 Score: 0.6814\n",
      "Log Loss: 0.7728\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6923\n",
      "F1 Score: 0.6959\n",
      "Log Loss: 0.5639\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7399\n",
      "Log Loss: 0.4829\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7511\n",
      "Log Loss: 0.6565\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7949\n",
      "Log Loss: 0.4780\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.6883\n",
      "Log Loss: 0.4720\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7569\n",
      "Log Loss: 0.6293\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7658\n",
      "Log Loss: 0.5105\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0434\n",
      "[accuracy] Coefficient of Variation (CV): 5.79%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7170\n",
      "[f1_score] Metric Standard Deviation: 0.0548\n",
      "[f1_score] Coefficient of Variation (CV): 7.64%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.4986\n",
      "[log_loss] Metric Standard Deviation: 0.0231\n",
      "[log_loss] Coefficient of Variation (CV): 4.63%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7308\n",
      "[accuracy] Metric Standard Deviation: 0.0314\n",
      "[accuracy] Coefficient of Variation (CV): 4.30%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7253\n",
      "[f1_score] Metric Standard Deviation: 0.0301\n",
      "[f1_score] Coefficient of Variation (CV): 4.16%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.6904\n",
      "[log_loss] Metric Standard Deviation: 0.0491\n",
      "[log_loss] Coefficient of Variation (CV): 7.11%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0377\n",
      "[accuracy] Coefficient of Variation (CV): 5.03%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7496\n",
      "[f1_score] Metric Standard Deviation: 0.0340\n",
      "[f1_score] Coefficient of Variation (CV): 4.54%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5276\n",
      "[log_loss] Metric Standard Deviation: 0.0323\n",
      "[log_loss] Coefficient of Variation (CV): 6.13%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[255  21]\n",
      " [ 77  37]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.77      0.92      0.84       276\n",
      "          DD       0.64      0.32      0.43       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.62      0.63       390\n",
      "weighted avg       0.73      0.75      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[228  48]\n",
      " [ 57  57]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.80      0.83      0.81       276\n",
      "          DD       0.54      0.50      0.52       114\n",
      "\n",
      "    accuracy                           0.73       390\n",
      "   macro avg       0.67      0.66      0.67       390\n",
      "weighted avg       0.72      0.73      0.73       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[224  52]\n",
      " [ 46  68]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.83      0.81      0.82       276\n",
      "          DD       0.57      0.60      0.58       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.70      0.70       390\n",
      "weighted avg       0.75      0.75      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7487 ± 0.0434 | 0.7170 ± 0.0548 | 0.4986 ± 0.0231 |\n",
      "| XGBoost      | default                                                            | 0.7308 ± 0.0314 | 0.7253 ± 0.0301 | 0.6904 ± 0.0491 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20} | 0.7487 ± 0.0377 | 0.7496 ± 0.0340 | 0.5276 ± 0.0323 |\n",
      "\n",
      "Performing binary classification: PD vs HC with cost-sensitive learning...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2883\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8813\n",
      "Log Loss: 0.3418\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8593\n",
      "Log Loss: 0.3324\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8769\n",
      "Log Loss: 0.3196\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8169\n",
      "F1 Score: 0.8188\n",
      "Log Loss: 0.4179\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8310\n",
      "F1 Score: 0.8219\n",
      "Log Loss: 0.3193\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2830\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.3211\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2696\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8557\n",
      "Log Loss: 0.3710\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8516\n",
      "Log Loss: 0.3764\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8593\n",
      "Log Loss: 0.3031\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9287\n",
      "Log Loss: 0.2820\n",
      "Model results for XGBoost:\n",
      "Parameters: ('default',)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.8972\n",
      "Log Loss: 0.2647\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.8937\n",
      "Log Loss: 0.2313\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.8817\n",
      "[accuracy] Metric Standard Deviation: 0.0246\n",
      "[accuracy] Coefficient of Variation (CV): 2.79%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8796\n",
      "[f1_score] Metric Standard Deviation: 0.0254\n",
      "[f1_score] Coefficient of Variation (CV): 2.89%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3088\n",
      "[log_loss] Metric Standard Deviation: 0.0340\n",
      "[log_loss] Coefficient of Variation (CV): 11.02%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8676\n",
      "[accuracy] Metric Standard Deviation: 0.0290\n",
      "[accuracy] Coefficient of Variation (CV): 3.34%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8634\n",
      "[f1_score] Metric Standard Deviation: 0.0269\n",
      "[f1_score] Coefficient of Variation (CV): 3.11%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3444\n",
      "[log_loss] Metric Standard Deviation: 0.0516\n",
      "[log_loss] Coefficient of Variation (CV): 14.99%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8704\n",
      "[accuracy] Metric Standard Deviation: 0.0225\n",
      "[accuracy] Coefficient of Variation (CV): 2.59%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8605\n",
      "[f1_score] Metric Standard Deviation: 0.0230\n",
      "[f1_score] Coefficient of Variation (CV): 2.68%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2912\n",
      "[log_loss] Metric Standard Deviation: 0.0366\n",
      "[log_loss] Coefficient of Variation (CV): 12.55%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 3\n",
      "Models: ['RandomForest', 'XGBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 55  24]\n",
      " [ 18 258]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.70      0.72        79\n",
      "          PD       0.91      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.82      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: default\n",
      "Confusion Matrix:\n",
      "[[ 49  30]\n",
      " [ 17 259]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.74      0.62      0.68        79\n",
      "          PD       0.90      0.94      0.92       276\n",
      "\n",
      "    accuracy                           0.87       355\n",
      "   macro avg       0.82      0.78      0.80       355\n",
      "weighted avg       0.86      0.87      0.86       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 43  36]\n",
      " [ 10 266]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.81      0.54      0.65        79\n",
      "          PD       0.88      0.96      0.92       276\n",
      "\n",
      "    accuracy                           0.87       355\n",
      "   macro avg       0.85      0.75      0.79       355\n",
      "weighted avg       0.87      0.87      0.86       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}   | 0.8817 ± 0.0246 | 0.8796 ± 0.0254 | 0.3088 ± 0.0340 |\n",
      "| XGBoost      | default                                                            | 0.8676 ± 0.0290 | 0.8634 ± 0.0269 | 0.3444 ± 0.0516 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.8704 ± 0.0225 | 0.8605 ± 0.0230 | 0.2912 ± 0.0366 |\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Labels mapping for reference:\n",
    "# 0: HC\n",
    "# 1: PD\n",
    "# 2: DD\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD (Default Mode)\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD [Default Mode]...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0, DD (2) becomes 1\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "    \n",
    "    results_pd_dd_default = run_cv(X_pd_dd, y_pd_dd_binary, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_dd_default = evaluate_cv(results_pd_dd_default, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"\\nBinary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC (Default Mode)\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC [Default Mode]...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    results_pd_hc_default = run_cv(X_pd_hc, y_pd_hc, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_hc_default = evaluate_cv(results_pd_hc_default, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"\\nBinary classification (PD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD with SMOTE\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD with SMOTE...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0, DD (2) becomes 1\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "    \n",
    "    results_pd_dd_smote = run_cv(X_pd_dd, y_pd_dd_binary, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_dd_smote = evaluate_cv(results_pd_dd_smote, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC with SMOTE\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC with SMOTE...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    results_pd_hc_smote = run_cv(X_pd_hc, y_pd_hc, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_hc_smote = evaluate_cv(results_pd_hc_smote, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD with Cost-Sensitive Learning (Weighted Binary Mode)\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD with cost-sensitive learning...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0, DD (2) becomes 1\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "    \n",
    "    # Define custom class weights (assume PD is majority (0) and DD is minority (1))\n",
    "    class_weights_pd_dd = {0: 1.0, 1: 2.0}\n",
    "    \n",
    "    results_pd_dd_weighted = run_cv(X_pd_dd, y_pd_dd_binary, models, n_splits=5, mode=\"weighted_binary\",\n",
    "                                    class_weights=class_weights_pd_dd, tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_dd_weighted = evaluate_cv(results_pd_dd_weighted, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC with Cost-Sensitive Learning (Weighted Binary Mode)\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC with cost-sensitive learning...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    # Define custom class weights (assume HC is majority (0) and PD is minority (1))\n",
    "    class_weights_pd_hc = {0: 1.0, 1: 2.0}\n",
    "    \n",
    "    results_pd_hc_weighted = run_cv(X_pd_hc, y_pd_hc, models, n_splits=5, mode=\"weighted_binary\",\n",
    "                                    class_weights=class_weights_pd_hc, tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_hc_weighted = evaluate_cv(results_pd_hc_weighted, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs HC) is not possible with the current labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
