{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import tsfel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, f1_score, log_loss, confusion_matrix, classification_report)\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import lightgbm as lgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_robustness(metric_name, metric_values, threshold_percent=10):\n",
    "    \"\"\"\n",
    "    Check the robustness of a model based on k-fold metric values.\n",
    "    \n",
    "    A model is considered robust if the coefficient of variation (CV),\n",
    "    defined as (standard deviation / mean) * 100, is less than threshold_percent.\n",
    "    \n",
    "    Parameters:\n",
    "        metric_values (list or np.array): List/array of metric values from each fold.\n",
    "        threshold_percent (float): The maximum allowed CV percentage (default: 10).\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the model is robust (CV < threshold_percent), False otherwise.\n",
    "    \"\"\"\n",
    "    metric_values = np.array(metric_values)\n",
    "    mean_val = np.mean(metric_values)\n",
    "    std_val = np.std(metric_values)\n",
    "    # Avoid division by zero:\n",
    "    if mean_val == 0:\n",
    "        cv = float('inf')\n",
    "    else:\n",
    "        cv = (std_val / mean_val) * 100\n",
    "\n",
    "    print(f\"[{metric_name}] Metric Mean: {mean_val:.4f}\")\n",
    "    print(f\"[{metric_name}] Metric Standard Deviation: {std_val:.4f}\")\n",
    "    print(f\"[{metric_name}] Coefficient of Variation (CV): {cv:.2f}%\")\n",
    "    \n",
    "    if cv < threshold_percent:\n",
    "        print(f\"[{metric_name}] Model is robust (CV < {threshold_percent}%)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"[{metric_name}] Model is not robust (CV >= {threshold_percent}%)\")\n",
    "        return False\n",
    "    \n",
    "def load_movement_features(subject_id, base_path='../../data/preprocessed/movement/', sampling_rate=100):\n",
    "    \"\"\"\n",
    "    Load and structure movement data from binary files with proper validation\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): Subject identifier (e.g., '001')\n",
    "        base_path (str): Base directory for movement data\n",
    "        sampling_rate (int): Sampling rate in Hz (used for validation)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Structured data with keys as channel names and values as time series arrays\n",
    "    \"\"\"\n",
    "    # File path handling\n",
    "    path = Path(base_path) / f'{subject_id}_ml.bin'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Movement data not found for subject {subject_id}\")\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_data = np.fromfile(path, dtype=np.float32)\n",
    "    \n",
    "    # Channel configuration (from your specification)\n",
    "    tasks = [\"Relaxed1\", \"Relaxed2\", \"RelaxedTask1\", \"RelaxedTask2\", \n",
    "             \"StretchHold\", \"HoldWeight\", \"DrinkGlas\", \"CrossArms\", \n",
    "             \"TouchNose\", \"Entrainment1\", \"Entrainment2\"]\n",
    "    wrists = [\"Left\", \"Right\"]\n",
    "    sensors = [\"Accelerometer\", \"Gyroscope\"]\n",
    "    axes = [\"X\", \"Y\", \"Z\"]\n",
    "    \n",
    "    # Calculate expected parameters\n",
    "    n_channels = len(tasks) * len(wrists) * len(sensors) * len(axes)\n",
    "    # Even though the duration was 10.24 per assessment, the first ~0.5 seconds are not used and thus\n",
    "    # the preprocessed data has only 9.76 seconds of data\n",
    "    expected_duration = 9.76  # seconds per assessment\n",
    "    expected_timepoints = int(expected_duration * sampling_rate)  # 976\n",
    "    \n",
    "    # Validate data size\n",
    "    expected_size = n_channels * expected_timepoints\n",
    "    if len(raw_data) != expected_size:\n",
    "        raise ValueError(f\"Unexpected data size for {subject_id}: \"\n",
    "                         f\"Got {len(raw_data)} elements, expected {expected_size}\")\n",
    "    \n",
    "    # Reshape and structure the data\n",
    "    structured_data = {}\n",
    "    channel_idx = 0\n",
    "    \n",
    "    for task in tasks:\n",
    "        for wrist in wrists:\n",
    "            for sensor in sensors:\n",
    "                for axis in axes:\n",
    "                    # Extract channel data\n",
    "                    start = channel_idx * expected_timepoints\n",
    "                    end = (channel_idx + 1) * expected_timepoints\n",
    "                    \n",
    "                    # Create channel name\n",
    "                    channel_name = f\"{task}_{wrist}_{sensor.split(' ')[0]}_{axis}\"\n",
    "                    \n",
    "                    structured_data[channel_name] = raw_data[start:end]\n",
    "                    \n",
    "                    channel_idx += 1\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def extract_tsfel_ts_features(channel_data, domain=None, fs=100):\n",
    "    \"\"\"\n",
    "    Extract TSFEL features from a single-channel time series.\n",
    "    \n",
    "    Args:\n",
    "        channel_data (np.ndarray): 1D array of time series values.\n",
    "        domain: (str): Domain of features to extract (default: 'all').\n",
    "            - 'statistical', 'temporal', 'spectral', 'fractal': Includes the corresponding feature domain.\n",
    "            - 'all': Includes all available feature domains.\n",
    "            - list of str: A combination of the above strings, e.g., ['statistical', 'temporal'].\n",
    "            - None: By default, includes the 'statistical', 'temporal', and 'spectral' domains.\n",
    "        fs (int): Sampling frequency (default: 100).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of TSFEL features.\n",
    "    \"\"\"\n",
    "    # Obtain a default configuration covering all domains.\n",
    "    cfg = tsfel.get_features_by_domain(domain)\n",
    "\n",
    "    # Extract features; the result is a DataFrame with one row.\n",
    "    features_df = tsfel.time_series_features_extractor(cfg, channel_data, fs=fs, verbose=0)\n",
    "    # Flatten to 1D numpy array and return.\n",
    "    return features_df.values.flatten()\n",
    "\n",
    "def extract_ts_features(label, channel_data, domain=None, fs=None):\n",
    "    if label == 'basic':\n",
    "        features = [\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data),\n",
    "            np.min(channel_data),\n",
    "            np.max(channel_data),\n",
    "            np.percentile(channel_data, 25),\n",
    "            np.percentile(channel_data, 75),\n",
    "            np.var(channel_data),\n",
    "            len(np.where(np.diff(np.sign(channel_data)))[0]) / len(channel_data),  # Zero-crossing rate\n",
    "            np.sqrt(np.mean(channel_data**2))  # Root Mean Square (RMS)\n",
    "        ]\n",
    "        return features\n",
    "    else:\n",
    "        return extract_tsfel_ts_features(channel_data, domain=domain, fs=fs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "\n",
    "Requisite: Python 3.12 (por catboost). brew install libomp para xgboost\n",
    "\n",
    "En base a los datos preprocesados, entrenaremos modelos de clasificacion bajo distintos escenarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 102/469 [02:30<09:02,  1.48s/it]\n",
      "/Users/santiago/santi/uoc/TFG/pads-tfg/.venv/lib/python3.12/site-packages/tsfel/feature_extraction/calc_features.py:182: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"Extraction of time series features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label != \u001b[33m'\u001b[39m\u001b[33mquestionnaire-only\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m channel_name, channel_data \u001b[38;5;129;01min\u001b[39;00m movement_data.items():\n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# Extract features using the specified domain and sampling rate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         ts_features[channel_name] = \u001b[43mextract_ts_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdomain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# Concatenate features from all channels\u001b[39;00m\n\u001b[32m     64\u001b[39m     concat_ts_features = np.concatenate(\u001b[38;5;28mlist\u001b[39m(ts_features.values()), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mextract_ts_features\u001b[39m\u001b[34m(label, channel_data, domain, fs)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_tsfel_ts_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mextract_tsfel_ts_features\u001b[39m\u001b[34m(channel_data, domain, fs)\u001b[39m\n\u001b[32m    114\u001b[39m cfg = tsfel.get_features_by_domain(domain)\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Extract features; the result is a DataFrame with one row.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m features_df = \u001b[43mtsfel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime_series_features_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Flatten to 1D numpy array and return.\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m features_df.values.flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/santi/uoc/TFG/pads-tfg/.venv/lib/python3.12/site-packages/tsfel/feature_extraction/calc_features.py:377\u001b[39m, in \u001b[36mtime_series_features_extractor\u001b[39m\u001b[34m(config, timeseries, fs, window_size, overlap, verbose, **kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\n\u001b[32m    373\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_jobs value is not valid. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mChoose an integer value or None for no multiprocessing.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    374\u001b[39m         )\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# single window\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     features_final = \u001b[43mcalc_window_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# Assuring the same feature extraction order\u001b[39;00m\n\u001b[32m    388\u001b[39m features_final = features_final.reindex(\u001b[38;5;28msorted\u001b[39m(features_final.columns), axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/santi/uoc/TFG/pads-tfg/.venv/lib/python3.12/site-packages/tsfel/feature_extraction/calc_features.py:510\u001b[39m, in \u001b[36mcalc_window_features\u001b[39m\u001b[34m(config, window, fs, verbose, single_window, **kwargs)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Eval feature results\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_axis:\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m     eval_result = \u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_total\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparameters_total\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     eval_result = np.array([eval_result])\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(header_names)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/santi/uoc/TFG/pads-tfg/.venv/lib/python3.12/site-packages/tsfel/feature_extraction/features.py:1450\u001b[39m, in \u001b[36mmfcc\u001b[39m\u001b[34m(signal, fs, pre_emphasis, nfft, nfilt, num_ceps, cep_lifter)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;129m@set_domain\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mspectral\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1416\u001b[39m \u001b[38;5;129m@set_domain\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m\"\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33memg\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmfcc\u001b[39m(signal, fs, pre_emphasis=\u001b[32m0.97\u001b[39m, nfft=\u001b[32m512\u001b[39m, nfilt=\u001b[32m40\u001b[39m, num_ceps=\u001b[32m12\u001b[39m, cep_lifter=\u001b[32m22\u001b[39m):\n\u001b[32m   1418\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Computes the MEL cepstral coefficients.\u001b[39;00m\n\u001b[32m   1419\u001b[39m \n\u001b[32m   1420\u001b[39m \u001b[33;03m    It provides the information about the power in each frequency band.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1448\u001b[39m \u001b[33;03m        MEL cepstral coefficients\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m     filter_banks = \u001b[43mfilterbank\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_emphasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfilt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1452\u001b[39m     mel_coeff = scipy.fft.dct(filter_banks, \u001b[38;5;28mtype\u001b[39m=\u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, norm=\u001b[33m\"\u001b[39m\u001b[33mortho\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m1\u001b[39m : (num_ceps + \u001b[32m1\u001b[39m)]  \u001b[38;5;66;03m# Keep 2-13\u001b[39;00m\n\u001b[32m   1454\u001b[39m     mel_coeff -= np.mean(mel_coeff, axis=\u001b[32m0\u001b[39m) + \u001b[32m1e-8\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/santi/uoc/TFG/pads-tfg/.venv/lib/python3.12/site-packages/tsfel/feature_extraction/features_utils.py:153\u001b[39m, in \u001b[36mfilterbank\u001b[39m\u001b[34m(signal, fs, pre_emphasis, nfft, nfilt)\u001b[39m\n\u001b[32m    150\u001b[39m enorm = \u001b[32m2.0\u001b[39m / (hz_points[\u001b[32m2\u001b[39m : nfilt + \u001b[32m2\u001b[39m] - hz_points[:nfilt])\n\u001b[32m    151\u001b[39m fbank *= enorm[:, np.newaxis]\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m filter_banks = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpow_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfbank\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m filter_banks = np.where(\n\u001b[32m    155\u001b[39m     filter_banks == \u001b[32m0\u001b[39m,\n\u001b[32m    156\u001b[39m     np.finfo(\u001b[38;5;28mfloat\u001b[39m).eps,\n\u001b[32m    157\u001b[39m     filter_banks,\n\u001b[32m    158\u001b[39m )  \u001b[38;5;66;03m# Numerical Stability\u001b[39;00m\n\u001b[32m    159\u001b[39m filter_banks = \u001b[32m20\u001b[39m * np.log10(filter_banks)  \u001b[38;5;66;03m# dB\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cargar metadatos\n",
    "file_list = pd.read_csv('../../data/preprocessed/file_list.csv',  dtype={'id': str})\n",
    "\n",
    "movement_data_per_subject = {}\n",
    "sampling_rate = 100\n",
    "\n",
    "# For each channel, extract TSFEL features and overwrite the original data\n",
    "\n",
    "# Create multiple cases so we can check each model independently:\n",
    "# 1. Basic statistical features\n",
    "# 2. TSFEL temporal, statistical, and spectral features\n",
    "# 3. TSFEL only temporal features\n",
    "# 4. TSFEL only statistical features\n",
    "# 5. TSFEL only spectral features\n",
    "# 6. Only questionnaire data\n",
    "\n",
    "# pipeline_labels = ['basic', 'TSFEL-all', 'TSFEL-temporal', 'TSFEL-statistical', 'TSFEL-spectral', 'questionnaire-only']\n",
    "pipeline_labels = ['basic', 'TSFEL-spectral', 'questionnaire-only']\n",
    "\n",
    "pipeline_args = [\n",
    "    {'domain': None, 'fs': None},  # Basic statistical features\n",
    "    #{'domain': None, 'fs': sampling_rate},  # TSFEL-all\n",
    "    #{'domain': 'temporal', 'fs': sampling_rate},  # TSFEL-temporal\n",
    "    #{'domain': 'statistical', 'fs': sampling_rate},  # TSFEL-statistical\n",
    "    {'domain': 'spectral', 'fs': sampling_rate},  # TSFEL-spectral\n",
    "    None,  # Questionnaire-only\n",
    "]\n",
    "\n",
    "# X must be 2D (samples, features)\n",
    "X_vals = {label: [] for label in pipeline_labels}\n",
    "y_vals = {label: [] for label in pipeline_labels}\n",
    "\n",
    "# First, load timeseries data and extract features\n",
    "for _, row in tqdm(file_list.iterrows(), total=len(file_list)):\n",
    "    # Cargar cuestionario\n",
    "    quest_data = np.fromfile(f'../../data/preprocessed/questionnaire/{row[\"id\"]}_ml.bin', \n",
    "                           dtype=np.float32)\n",
    "    \n",
    "    # Load structured movement data\n",
    "    movement_data = load_movement_features(row[\"id\"])\n",
    "\n",
    "    \"\"\"\n",
    "    movement_data:\n",
    "    {\n",
    "        'Relaxed1_Left_Accelerometer_X': np.array([...]),\n",
    "        'Relaxed1_Left_Accelerometer_Y': np.array([...]),\n",
    "        ...\n",
    "    }\n",
    "    Each key corresponds to a channel, and the value is a 1D numpy array of time series data.\n",
    "    We have a total of 132 keys and each time series has 976 time points.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for label, args in zip(pipeline_labels, pipeline_args):\n",
    "        # Extract features for each channel unless label is 'questionnaire-only'\n",
    "        features = quest_data\n",
    "        ts_features = {}\n",
    "        if label != 'questionnaire-only':\n",
    "            for channel_name, channel_data in movement_data.items():\n",
    "                # Extract features using the specified domain and sampling rate\n",
    "                ts_features[channel_name] = extract_ts_features(label, channel_data, domain=args['domain'], fs=args['fs'])\n",
    "\n",
    "            # Concatenate features from all channels\n",
    "            concat_ts_features = np.concatenate(list(ts_features.values()), axis=0)\n",
    "\n",
    "            # Combine questionnaire data with time series features\n",
    "            features = np.concatenate((features, concat_ts_features), axis=0)\n",
    "        \n",
    "        X_vals[label].append(features.reshape(1, -1))\n",
    "        y_vals[label].append(row['label'])\n",
    "\n",
    "\n",
    "# Combine all samples for each pipeline\n",
    "for label in pipeline_labels:\n",
    "    if X_vals[label]:\n",
    "        X_vals[label] = np.concatenate(X_vals[label], axis=0)\n",
    "        y_vals[label] = np.array(y_vals[label])\n",
    "    else:\n",
    "        X_vals[label] = np.array([])\n",
    "        y_vals[label] = np.array([])\n",
    "\n",
    "for x in X_vals:\n",
    "    print(f\"{x} shape: {X_vals[x].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Labels mapping for reference:\n",
    "# 0: HC\n",
    "# 1: PD\n",
    "# 2: DD\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def tune_models(clf, param_grids, X_inner, y_inner, model_name, fit_params=None):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for each model using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "        clf (sklearn classifier): Classifier instance to tune.\n",
    "        param_grids (dict): Dictionary mapping model names to their hyperparameter grids.\n",
    "        X_inner (np.array or pd.DataFrame): Feature matrix for inner CV.\n",
    "        y_inner (np.array): Labels for inner CV.\n",
    "        model_name (str): Name of the model being tuned.\n",
    "        fit_params (dict): Additional parameters for fitting the model.\n",
    "        \n",
    "    Returns:\n",
    "        tuned_results (dict): Dictionary mapping model names to the fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    # Use GridSearchCV with our custom refit_strategy.\n",
    "    scoring = {\"accuracy\": \"accuracy\", \"f1_weighted\": \"f1_weighted\"}\n",
    "    grid = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=param_grids[model_name],\n",
    "        scoring=scoring,\n",
    "        cv=3,       # inner CV folds\n",
    "        n_jobs=-1,\n",
    "        refit=refit_strategy  # custom refit to pick robust candidate\n",
    "    )\n",
    "    grid.fit(X_inner, y_inner, **fit_params)\n",
    "    return (grid.best_estimator_, grid.best_params_)\n",
    "\n",
    "def refit_strategy(cv_results):\n",
    "    \"\"\"\n",
    "    Custom refit strategy that uses both accuracy and weighted F1 metrics.\n",
    "    \n",
    "    For each hyperparameter candidate (as provided in GridSearchCV’s cv_results_),\n",
    "    we compute the mean and standard deviation for both 'accuracy' and 'f1_weighted'.\n",
    "    We then calculate the coefficient of variation (CV) for each metric in percentage.\n",
    "    The composite score is calculated as:\n",
    "    \n",
    "        composite = 0.5*(mean_accuracy + mean_f1_weighted) - λ * 0.5*(CV_accuracy + CV_f1_weighted)\n",
    "    \n",
    "    A candidate with a high mean score and a low CV will be preferred.\n",
    "    \n",
    "    Parameters:\n",
    "        cv_results (dict): The cv_results_ dictionary from GridSearchCV. It must contain the keys:\n",
    "            \"mean_test_accuracy\", \"std_test_accuracy\", \n",
    "            \"mean_test_f1_weighted\", \"std_test_f1_weighted\".\n",
    "    \n",
    "    Returns:\n",
    "        best_index (int): The index (as in cv_results) of the best candidate.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    required_keys = [\"mean_test_accuracy\", \"std_test_accuracy\", \"mean_test_f1_weighted\", \"std_test_f1_weighted\"]\n",
    "    if not all(key in df.columns for key in required_keys):\n",
    "        raise ValueError(f\"cv_results must contain the keys: {required_keys}\")\n",
    "    \n",
    "    # Means and standard deviations for accuracy and weighted F1.\n",
    "    mean_acc = df[\"mean_test_accuracy\"]\n",
    "    std_acc  = df[\"std_test_accuracy\"]\n",
    "    mean_f1  = df[\"mean_test_f1_weighted\"]\n",
    "    std_f1   = df[\"std_test_f1_weighted\"]\n",
    "    \n",
    "    # Compute coefficient of variation (CV) in percentages.\n",
    "    cv_acc = (std_acc / mean_acc) * 100\n",
    "    cv_f1  = (std_f1 / mean_f1) * 100\n",
    "    \n",
    "    # Lambda controls the weight given to robustness. Adjust as needed.\n",
    "    lambda_val = 0.01\n",
    "    \n",
    "    # Composite score: we want high mean and low CV.\n",
    "    composite = 0.5 * (mean_acc + mean_f1) - lambda_val * 0.5 * (cv_acc + cv_f1)\n",
    "\n",
    "    best_index = composite.idxmax()\n",
    "    return best_index\n",
    "\n",
    "def run_cv(X, y, models, n_splits=5, mode=\"default\", class_weights=None, tune_inner=False, param_grids=None):\n",
    "    \"\"\"\n",
    "    Unified cross-validation runner with optional inner-loop hyperparameter tuning.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.array or pd.DataFrame): Feature matrix.\n",
    "        y (np.array): Labels.\n",
    "        models (dict): Dictionary mapping model names to a tuple: (classifier instance, fit_params dict).\n",
    "                       (If no additional fit parameters are needed, use an empty dict.)\n",
    "        n_splits (int): Number of outer CV folds.\n",
    "        mode (str): One of:\n",
    "            - \"default\": standard CV.\n",
    "            - \"smote\": apply SMOTE oversampling on the training data.\n",
    "            - \"weighted\": for multi-class cost-sensitive learning.\n",
    "            - \"weighted_binary\": for binary cost-sensitive learning (with special handling for XGBoost).\n",
    "        class_weights (dict): Custom class weighting dictionary (e.g., {0: 1.0, 1: 2.0}).\n",
    "        tune_inner (bool): If True and param_grids is provided, perform inner-loop GridSearchCV tuning.\n",
    "        param_grids (dict): Dictionary mapping model names to their hyperparameter grids for tuning.\n",
    "                           Only used if tune_inner is True.\n",
    "        \n",
    "    Returns:\n",
    "        dict: For each model, a dictionary with keys:\n",
    "            \"accuracy\": list of accuracies per outer fold,\n",
    "            \"f1_score\": list of weighted F1 scores per outer fold,\n",
    "            \"log_loss\": list of log losses per outer fold,\n",
    "            \"y_true\": list of true label arrays per fold,\n",
    "            \"y_pred\": list of predicted label arrays per fold,\n",
    "            \"y_pred_proba\": list of predicted probability arrays per fold.\n",
    "    \"\"\"\n",
    "    # Suppress warnings regarding use_label_encoder and feature names\n",
    "    # Ensure X is a DataFrame with valid feature names.\n",
    "    if not hasattr(X, \"columns\"):\n",
    "        X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Store results as an array of \"folds\" for each model.\n",
    "    results = { name: [] for name in models.keys() }\n",
    "\n",
    "    # Initialize SMOTE if selected.\n",
    "    if mode == \"smote\":\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        oversampler = SMOTE(random_state=42)\n",
    "\n",
    "    # Outer CV loop.\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Optionally apply SMOTE.\n",
    "        if mode == \"smote\":\n",
    "            X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # For each model:\n",
    "        for name, (model, fit_params) in models.items():\n",
    "            clf = clone(model)\n",
    "            fold_fit_params = fit_params.copy() if fit_params is not None else {}\n",
    "\n",
    "            # For weighted modes, set class_weight if supported.\n",
    "            if mode in (\"weighted\", \"weighted_binary\") and class_weights is not None:\n",
    "                if 'class_weight' in clf.get_params():\n",
    "                    clf.set_params(class_weight=class_weights)\n",
    "\n",
    "            # Special handling for XGBoost in weighted_binary mode.\n",
    "            if mode == \"weighted_binary\":\n",
    "                try:\n",
    "                    from xgboost import XGBClassifier\n",
    "                    if name == \"XGBoost\" and isinstance(clf, XGBClassifier) and len(np.unique(y_train)) == 2:\n",
    "                        ratio = class_weights.get(1, 1.0) / class_weights.get(0, 1.0)\n",
    "                        clf.set_params(objective='binary:logistic', scale_pos_weight=ratio)\n",
    "                except ImportError:\n",
    "                    pass\n",
    "\n",
    "            # For LightGBM: enforce early stopping parameters if not provided and set verbosity to -1.\n",
    "            if name == \"LightGBM\":\n",
    "                if \"eval_set\" not in fold_fit_params:\n",
    "                    fold_fit_params[\"eval_set\"] = [(X_test, y_test)]\n",
    "                if \"eval_metric\" not in fold_fit_params:\n",
    "                    fold_fit_params[\"eval_metric\"] = \"logloss\"\n",
    "                clf.set_params(verbose=-1)\n",
    "\n",
    "            # --- Inner-loop: Hyperparameter tuning ---\n",
    "            hyperparameter_tuning_best_params = None\n",
    "            if tune_inner and param_grids is not None and name in param_grids:\n",
    "                # Perform hyperparameter tuning using GridSearchCV.\n",
    "                print(f\"Tuning hyperparameters for {name}...\")\n",
    "                (best_estimator, best_params) = tune_models(clf, param_grids, X_train, y_train, name, fit_params=fold_fit_params)\n",
    "                clf = best_estimator\n",
    "                hyperparameter_tuning_best_params = best_params\n",
    "            else:\n",
    "                clf.fit(X_train, y_train, **fold_fit_params)\n",
    "\n",
    "            # Evaluate on the outer test set.\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "            # Store each model's results in the \"results\" array, where each outer fold is indexed by k_idx.\n",
    "            # And each element in the results array is a metrics dictionary.\n",
    "            model_metrics = {}\n",
    "            \n",
    "            model_metrics[\"params\"] = hyperparameter_tuning_best_params if hyperparameter_tuning_best_params else \"default\",\n",
    "            model_metrics[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "            model_metrics[\"f1_score\"] = f1_score(y_test, y_pred, average='weighted')\n",
    "            model_metrics[\"log_loss\"] = log_loss(y_test, y_pred_proba)\n",
    "            model_metrics[\"y_true\"] = y_test\n",
    "            model_metrics[\"y_pred\"] = y_pred\n",
    "            model_metrics[\"y_pred_proba\"] = y_pred_proba\n",
    "\n",
    "            print(f\"Model results for {name}:\")\n",
    "            print(f\"Parameters: {model_metrics['params']}\")\n",
    "            print(f\"Accuracy: {model_metrics['accuracy']:.4f}\")\n",
    "            print(f\"F1 Score: {model_metrics['f1_score']:.4f}\")\n",
    "            print(f\"Log Loss: {model_metrics['log_loss']:.4f}\")\n",
    "            \n",
    "            results[name].append(model_metrics)\n",
    "\n",
    "    # Check robustness for each model (using your check_robustness function)\n",
    "    for name in results.keys():\n",
    "        # The metrics stored for each model are in an array of dictionaries, where each dictionary corresponds to a fold.\n",
    "        # We need to extract the metrics from each fold and check their robustness.\n",
    "        acc_values = [results[name][k][\"accuracy\"] for k in range(n_splits)]\n",
    "        f1_values = [results[name][k][\"f1_score\"] for k in range(n_splits)]\n",
    "        ll_values = [results[name][k][\"log_loss\"] for k in range(n_splits)]\n",
    "\n",
    "        print(f\"\\nRobustness for model: {name}\")\n",
    "        acc_robust = check_robustness(\"accuracy\", acc_values)\n",
    "        f1_robust = check_robustness(\"f1_score\", f1_values)\n",
    "        ll_robust = check_robustness(\"log_loss\", ll_values)\n",
    "        if acc_robust and f1_robust and ll_robust:\n",
    "            print(f\"{name} is robust across folds.\\n\")\n",
    "        else:\n",
    "            print(f\"[ERROR] {name} is not robust across folds.\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def evaluate_cv(results, target_names):\n",
    "    \"\"\"\n",
    "    Aggregates per-fold metrics from a results dictionary and prints overall metrics,\n",
    "    confusion matrix, and classification report using tabulate.\n",
    "    \n",
    "    Parameters:\n",
    "        results (dict): Dictionary mapping model names to a list of per-fold metric dictionaries.\n",
    "                        Each per-fold dictionary must include:\n",
    "                            \"accuracy\": float,\n",
    "                            \"f1_score\": float,\n",
    "                            \"log_loss\": float,\n",
    "                            \"y_true\": true labels for the fold,\n",
    "                            \"y_pred\": predicted labels for the fold,\n",
    "                            \"y_pred_proba\": predicted probability array for the fold,\n",
    "                            \"params\": the model parameters used.\n",
    "        target_names (list): List of class names (e.g., ['HC', 'PD', 'DD']).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of model names to overall metrics including:\n",
    "            \"accuracy_mean\", \"accuracy_std\", \"f1_mean\", \"f1_std\", \n",
    "            \"log_loss_mean\", \"log_loss_std\", \"confusion_matrix\", and \"classification_report\".\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    overall_metrics = {}\n",
    "    table_data = []\n",
    "\n",
    "    # From target_names, print what we are evaluating\n",
    "    print(f\"Evaluating models with target names: {target_names}\")\n",
    "    print(f\"Number of classes: {len(target_names)}\")\n",
    "    print(f\"Classes: {target_names}\")\n",
    "    print(f\"Number of models: {len(results)}\")\n",
    "    print(f\"Models: {list(results.keys())}\")\n",
    "\n",
    "    for name, fold_results in results.items():\n",
    "        # Aggregate per-fold predictions.\n",
    "        y_true_all = np.concatenate([fold[\"y_true\"] for fold in fold_results])\n",
    "        y_pred_all = np.concatenate([fold[\"y_pred\"] for fold in fold_results])\n",
    "        y_pred_proba_all = np.concatenate([fold[\"y_pred_proba\"] for fold in fold_results])\n",
    "        \n",
    "        # Aggregate per-fold metric values.\n",
    "        acc_values = np.array([fold[\"accuracy\"] for fold in fold_results])\n",
    "        f1_values  = np.array([fold[\"f1_score\"] for fold in fold_results])\n",
    "        ll_values  = np.array([fold[\"log_loss\"] for fold in fold_results])\n",
    "        \n",
    "        # Retrieve parameters (assumed constant across folds).\n",
    "        params_val = fold_results[0].get(\"params\", \"default\")\n",
    "        if isinstance(params_val, tuple) and len(params_val) == 1:\n",
    "            params_val = params_val[0]\n",
    "        \n",
    "        # Compute mean and standard deviation for numeric metrics.\n",
    "        acc_mean, acc_std = np.mean(acc_values), np.std(acc_values)\n",
    "        f1_mean,  f1_std  = np.mean(f1_values),  np.std(f1_values)\n",
    "        ll_mean,  ll_std  = np.mean(ll_values),  np.std(ll_values)\n",
    "        \n",
    "        # Compute the overall confusion matrix and classification report.\n",
    "        cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "        clf_report = classification_report(y_true_all, y_pred_all, target_names=target_names, zero_division=0)\n",
    "        \n",
    "        overall_metrics[name] = {\n",
    "            \"params\": params_val,\n",
    "            \"accuracy_mean\": acc_mean,\n",
    "            \"accuracy_std\": acc_std,\n",
    "            \"f1_mean\": f1_mean,\n",
    "            \"f1_std\": f1_std,\n",
    "            \"log_loss_mean\": ll_mean,\n",
    "            \"log_loss_std\": ll_std,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"classification_report\": clf_report,\n",
    "        }\n",
    "        table_data.append([\n",
    "            name,\n",
    "            params_val,\n",
    "            f\"{acc_mean:.4f} ± {acc_std:.4f}\",\n",
    "            f\"{f1_mean:.4f} ± {f1_std:.4f}\",\n",
    "            f\"{ll_mean:.4f} ± {ll_std:.4f}\"\n",
    "        ])\n",
    "        \n",
    "        # Print detailed report for this model.\n",
    "        print(f\"Model: {name}\")\n",
    "        print(f\"Parameters: {params_val}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(clf_report)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Overall Metrics:\")\n",
    "    print(tabulate(table_data, headers=[\"Model\", \"Params\", \"Accuracy\", \"Weighted F1\", \"Log Loss\"], tablefmt=\"pipe\"))\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 300],#, 500],\n",
    "        \"max_depth\": [None, 10],#, 20],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"learning_rate\": [0.01, 0.05],#, 0.1, 0.3],\n",
    "        \"max_depth\": [3, 6],#, 9, 12],\n",
    "        \"subsample\": [0.5, 0.7],#, 1.0]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"depth\": [4, 6],#, 8, 10],\n",
    "        \"learning_rate\": [0.01],#, 0.03, 0.1],\n",
    "        \"l2_leaf_reg\": [1, 3],#, 5, 10]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"num_leaves\": [20, 31],#, 50, 100],\n",
    "        \"learning_rate\": [0.01, 0.05],#, 0.1, 0.2],\n",
    "        \"min_child_samples\": [10, 20],#, 30, 50]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "     \"RandomForest\": [RandomForestClassifier(random_state=42), {}],\n",
    "     \"XGBoost\": [XGBClassifier(eval_metric='mlogloss', random_state=42), {}],\n",
    "     \"CatBoost\": [CatBoostClassifier(verbose=0, random_state=42), {}],\n",
    "     \"LightGBM\": [LGBMClassifier(random_state=42), {'callbacks': [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)]}],\n",
    "#     \"LightGBM\": [LGBMClassifier(random_state=42), {}]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multiclase (PD vs DD vs HC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Performing multi-class classification (PD vs DD vs HC) [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.5988\n",
      "Log Loss: 0.7920\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6162\n",
      "Log Loss: 0.8610\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6318\n",
      "Log Loss: 0.7582\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6489\n",
      "F1 Score: 0.5637\n",
      "Log Loss: 0.7821\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.5945\n",
      "Log Loss: 0.7009\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6561\n",
      "Log Loss: 0.8189\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6699\n",
      "Log Loss: 0.6621\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6279\n",
      "Log Loss: 0.6988\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6186\n",
      "Log Loss: 1.1301\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6649\n",
      "Log Loss: 0.6917\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6207\n",
      "Log Loss: 0.6850\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6206\n",
      "Log Loss: 0.7105\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6004\n",
      "Log Loss: 0.7493\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6223\n",
      "Log Loss: 0.7953\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6004\n",
      "Log Loss: 0.7913\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.5992\n",
      "Log Loss: 0.7543\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6035\n",
      "Log Loss: 0.6956\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6184\n",
      "Log Loss: 0.8240\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6419\n",
      "Log Loss: 0.7272\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6882\n",
      "F1 Score: 0.6189\n",
      "Log Loss: 0.7608\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0161\n",
      "[accuracy] Coefficient of Variation (CV): 2.45%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6032\n",
      "[f1_score] Metric Standard Deviation: 0.0082\n",
      "[f1_score] Coefficient of Variation (CV): 1.36%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.8136\n",
      "[log_loss] Metric Standard Deviation: 0.1621\n",
      "[log_loss] Coefficient of Variation (CV): 19.93%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6780\n",
      "[accuracy] Metric Standard Deviation: 0.0240\n",
      "[accuracy] Coefficient of Variation (CV): 3.54%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6356\n",
      "[f1_score] Metric Standard Deviation: 0.0206\n",
      "[f1_score] Coefficient of Variation (CV): 3.25%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7982\n",
      "[log_loss] Metric Standard Deviation: 0.0572\n",
      "[log_loss] Coefficient of Variation (CV): 7.17%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.6717\n",
      "[accuracy] Metric Standard Deviation: 0.0244\n",
      "[accuracy] Coefficient of Variation (CV): 3.64%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6329\n",
      "[f1_score] Metric Standard Deviation: 0.0231\n",
      "[f1_score] Coefficient of Variation (CV): 3.64%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7248\n",
      "[log_loss] Metric Standard Deviation: 0.0471\n",
      "[log_loss] Coefficient of Variation (CV): 6.49%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6568\n",
      "[accuracy] Metric Standard Deviation: 0.0195\n",
      "[accuracy] Coefficient of Variation (CV): 2.98%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6060\n",
      "[f1_score] Metric Standard Deviation: 0.0232\n",
      "[f1_score] Coefficient of Variation (CV): 3.83%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7413\n",
      "[log_loss] Metric Standard Deviation: 0.0315\n",
      "[log_loss] Coefficient of Variation (CV): 4.25%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD', 'DD']\n",
      "Number of classes: 3\n",
      "Classes: ['HC', 'PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 47  20  12]\n",
      " [ 17 249  10]\n",
      " [ 27  75  12]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.52      0.59      0.55        79\n",
      "          PD       0.72      0.90      0.80       276\n",
      "          DD       0.35      0.11      0.16       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.53      0.53      0.51       469\n",
      "weighted avg       0.60      0.66      0.61       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 50  19  10]\n",
      " [ 15 249  12]\n",
      " [ 31  64  19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.52      0.63      0.57        79\n",
      "          PD       0.75      0.90      0.82       276\n",
      "          DD       0.46      0.17      0.25       114\n",
      "\n",
      "    accuracy                           0.68       469\n",
      "   macro avg       0.58      0.57      0.55       469\n",
      "weighted avg       0.64      0.68      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 50  18  11]\n",
      " [ 11 246  19]\n",
      " [ 27  68  19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.57      0.63      0.60        79\n",
      "          PD       0.74      0.89      0.81       276\n",
      "          DD       0.39      0.17      0.23       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.57      0.56      0.55       469\n",
      "weighted avg       0.63      0.67      0.63       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 42  24  13]\n",
      " [  9 252  15]\n",
      " [ 25  75  14]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.55      0.53      0.54        79\n",
      "          PD       0.72      0.91      0.80       276\n",
      "          DD       0.33      0.12      0.18       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.53      0.52      0.51       469\n",
      "weighted avg       0.60      0.66      0.61       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}   | 0.6568 ± 0.0161 | 0.6032 ± 0.0082 | 0.8136 ± 0.1621 |\n",
      "| XGBoost      | {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7}          | 0.6780 ± 0.0240 | 0.6356 ± 0.0206 | 0.7982 ± 0.0572 |\n",
      "| CatBoost     | {'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01}              | 0.6717 ± 0.0244 | 0.6329 ± 0.0231 | 0.7248 ± 0.0471 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20} | 0.6568 ± 0.0195 | 0.6060 ± 0.0232 | 0.7413 ± 0.0315 |\n",
      "Performing multi-class classification (PD vs DD vs HC) with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.6915\n",
      "F1 Score: 0.6386\n",
      "Log Loss: 0.7944\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6489\n",
      "F1 Score: 0.6278\n",
      "Log Loss: 0.8734\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6496\n",
      "Log Loss: 0.7812\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6219\n",
      "Log Loss: 0.8501\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6915\n",
      "F1 Score: 0.6706\n",
      "Log Loss: 0.6963\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7234\n",
      "F1 Score: 0.7090\n",
      "Log Loss: 0.8173\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7234\n",
      "F1 Score: 0.7068\n",
      "Log Loss: 0.6902\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6739\n",
      "Log Loss: 0.7061\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6489\n",
      "F1 Score: 0.6146\n",
      "Log Loss: 1.4983\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6064\n",
      "F1 Score: 0.5914\n",
      "Log Loss: 0.8667\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6472\n",
      "Log Loss: 0.7440\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6443\n",
      "Log Loss: 0.7871\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6229\n",
      "Log Loss: 1.1550\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6597\n",
      "Log Loss: 0.8180\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6230\n",
      "Log Loss: 0.8384\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6680\n",
      "Log Loss: 0.7741\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6325\n",
      "Log Loss: 0.7088\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6559\n",
      "F1 Score: 0.6387\n",
      "Log Loss: 0.8388\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6362\n",
      "Log Loss: 0.7412\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6452\n",
      "F1 Score: 0.6315\n",
      "Log Loss: 0.8184\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6695\n",
      "[accuracy] Metric Standard Deviation: 0.0220\n",
      "[accuracy] Coefficient of Variation (CV): 3.29%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6358\n",
      "[f1_score] Metric Standard Deviation: 0.0192\n",
      "[f1_score] Coefficient of Variation (CV): 3.02%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.9706\n",
      "[log_loss] Metric Standard Deviation: 0.3122\n",
      "[log_loss] Coefficient of Variation (CV): 32.16%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6610\n",
      "[accuracy] Metric Standard Deviation: 0.0378\n",
      "[accuracy] Coefficient of Variation (CV): 5.71%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6453\n",
      "[f1_score] Metric Standard Deviation: 0.0388\n",
      "[f1_score] Coefficient of Variation (CV): 6.01%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.8428\n",
      "[log_loss] Metric Standard Deviation: 0.0236\n",
      "[log_loss] Coefficient of Variation (CV): 2.80%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.6738\n",
      "[accuracy] Metric Standard Deviation: 0.0276\n",
      "[accuracy] Coefficient of Variation (CV): 4.09%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6526\n",
      "[f1_score] Metric Standard Deviation: 0.0287\n",
      "[f1_score] Coefficient of Variation (CV): 4.40%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7590\n",
      "[log_loss] Metric Standard Deviation: 0.0492\n",
      "[log_loss] Coefficient of Variation (CV): 6.48%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6567\n",
      "[accuracy] Metric Standard Deviation: 0.0187\n",
      "[accuracy] Coefficient of Variation (CV): 2.85%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6479\n",
      "[f1_score] Metric Standard Deviation: 0.0202\n",
      "[f1_score] Coefficient of Variation (CV): 3.12%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7872\n",
      "[log_loss] Metric Standard Deviation: 0.0483\n",
      "[log_loss] Coefficient of Variation (CV): 6.14%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD', 'DD']\n",
      "Number of classes: 3\n",
      "Classes: ['HC', 'PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "Confusion Matrix:\n",
      "[[ 53  12  14]\n",
      " [ 20 240  16]\n",
      " [ 30  63  21]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.51      0.67      0.58        79\n",
      "          PD       0.76      0.87      0.81       276\n",
      "          DD       0.41      0.18      0.25       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.56      0.57      0.55       469\n",
      "weighted avg       0.64      0.67      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 55  12  12]\n",
      " [ 26 225  25]\n",
      " [ 32  52  30]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.49      0.70      0.57        79\n",
      "          PD       0.78      0.82      0.80       276\n",
      "          DD       0.45      0.26      0.33       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.59      0.57       469\n",
      "weighted avg       0.65      0.66      0.65       469\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 54  10  15]\n",
      " [ 16 234  26]\n",
      " [ 27  59  28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.56      0.68      0.61        79\n",
      "          PD       0.77      0.85      0.81       276\n",
      "          DD       0.41      0.25      0.31       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.58      0.59      0.58       469\n",
      "weighted avg       0.65      0.67      0.65       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31}\n",
      "Confusion Matrix:\n",
      "[[ 52  11  16]\n",
      " [ 20 221  35]\n",
      " [ 29  50  35]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.51      0.66      0.58        79\n",
      "          PD       0.78      0.80      0.79       276\n",
      "          DD       0.41      0.31      0.35       114\n",
      "\n",
      "    accuracy                           0.66       469\n",
      "   macro avg       0.57      0.59      0.57       469\n",
      "weighted avg       0.65      0.66      0.65       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}     | 0.6695 ± 0.0220 | 0.6358 ± 0.0192 | 0.9706 ± 0.3122 |\n",
      "| XGBoost      | {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7}          | 0.6610 ± 0.0378 | 0.6453 ± 0.0388 | 0.8428 ± 0.0236 |\n",
      "| CatBoost     | {'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01}              | 0.6738 ± 0.0276 | 0.6526 ± 0.0287 | 0.7590 ± 0.0492 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 31} | 0.6567 ± 0.0187 | 0.6479 ± 0.0202 | 0.7872 ± 0.0483 |\n",
      "Performing multi-class classification (PD vs DD vs HC) with cost-sensitive learning...\n",
      "Computed class weights: {0: 1.978902953586498, 1: 0.5664251207729468, 2: 1.371345029239766}\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6048\n",
      "Log Loss: 1.1612\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6162\n",
      "Log Loss: 0.8610\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6809\n",
      "F1 Score: 0.6318\n",
      "Log Loss: 0.7582\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6064\n",
      "F1 Score: 0.6188\n",
      "Log Loss: 0.8044\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.7340\n",
      "F1 Score: 0.6766\n",
      "Log Loss: 0.7043\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6561\n",
      "Log Loss: 0.8189\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6699\n",
      "Log Loss: 0.6621\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6596\n",
      "F1 Score: 0.6611\n",
      "Log Loss: 0.7376\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.5957\n",
      "F1 Score: 0.5568\n",
      "Log Loss: 0.8269\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7021\n",
      "F1 Score: 0.6649\n",
      "Log Loss: 0.6917\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6207\n",
      "Log Loss: 0.6850\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.5745\n",
      "F1 Score: 0.5733\n",
      "Log Loss: 0.8100\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.6702\n",
      "F1 Score: 0.6467\n",
      "Log Loss: 0.7579\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6223\n",
      "Log Loss: 0.7953\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6277\n",
      "F1 Score: 0.6004\n",
      "Log Loss: 0.7913\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.6383\n",
      "F1 Score: 0.6492\n",
      "Log Loss: 0.7976\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.6989\n",
      "F1 Score: 0.6366\n",
      "Log Loss: 0.6846\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6184\n",
      "Log Loss: 0.8240\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.6774\n",
      "F1 Score: 0.6419\n",
      "Log Loss: 0.7272\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6344\n",
      "F1 Score: 0.6327\n",
      "Log Loss: 0.8283\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.6717\n",
      "[accuracy] Metric Standard Deviation: 0.0459\n",
      "[accuracy] Coefficient of Variation (CV): 6.84%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6243\n",
      "[f1_score] Metric Standard Deviation: 0.0408\n",
      "[f1_score] Coefficient of Variation (CV): 6.54%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.8270\n",
      "[log_loss] Metric Standard Deviation: 0.1742\n",
      "[log_loss] Coefficient of Variation (CV): 21.07%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.6780\n",
      "[accuracy] Metric Standard Deviation: 0.0240\n",
      "[accuracy] Coefficient of Variation (CV): 3.54%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6356\n",
      "[f1_score] Metric Standard Deviation: 0.0206\n",
      "[f1_score] Coefficient of Variation (CV): 3.25%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7982\n",
      "[log_loss] Metric Standard Deviation: 0.0572\n",
      "[log_loss] Coefficient of Variation (CV): 7.17%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.6717\n",
      "[accuracy] Metric Standard Deviation: 0.0244\n",
      "[accuracy] Coefficient of Variation (CV): 3.64%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6329\n",
      "[f1_score] Metric Standard Deviation: 0.0231\n",
      "[f1_score] Coefficient of Variation (CV): 3.64%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7248\n",
      "[log_loss] Metric Standard Deviation: 0.0471\n",
      "[log_loss] Coefficient of Variation (CV): 6.49%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.6226\n",
      "[accuracy] Metric Standard Deviation: 0.0294\n",
      "[accuracy] Coefficient of Variation (CV): 4.73%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.6270\n",
      "[f1_score] Metric Standard Deviation: 0.0304\n",
      "[f1_score] Coefficient of Variation (CV): 4.85%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.7956\n",
      "[log_loss] Metric Standard Deviation: 0.0307\n",
      "[log_loss] Coefficient of Variation (CV): 3.86%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD', 'DD']\n",
      "Number of classes: 3\n",
      "Classes: ['HC', 'PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 56  12  11]\n",
      " [ 20 245  11]\n",
      " [ 29  71  14]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.53      0.71      0.61        79\n",
      "          PD       0.75      0.89      0.81       276\n",
      "          DD       0.39      0.12      0.19       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.56      0.57      0.54       469\n",
      "weighted avg       0.62      0.67      0.63       469\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 50  19  10]\n",
      " [ 15 249  12]\n",
      " [ 31  64  19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.52      0.63      0.57        79\n",
      "          PD       0.75      0.90      0.82       276\n",
      "          DD       0.46      0.17      0.25       114\n",
      "\n",
      "    accuracy                           0.68       469\n",
      "   macro avg       0.58      0.57      0.55       469\n",
      "weighted avg       0.64      0.68      0.64       469\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 50  18  11]\n",
      " [ 11 246  19]\n",
      " [ 27  68  19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.57      0.63      0.60        79\n",
      "          PD       0.74      0.89      0.81       276\n",
      "          DD       0.39      0.17      0.23       114\n",
      "\n",
      "    accuracy                           0.67       469\n",
      "   macro avg       0.57      0.56      0.55       469\n",
      "weighted avg       0.63      0.67      0.63       469\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 58   8  13]\n",
      " [ 32 191  53]\n",
      " [ 31  40  43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.48      0.73      0.58        79\n",
      "          PD       0.80      0.69      0.74       276\n",
      "          DD       0.39      0.38      0.39       114\n",
      "\n",
      "    accuracy                           0.62       469\n",
      "   macro avg       0.56      0.60      0.57       469\n",
      "weighted avg       0.65      0.62      0.63       469\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}   | 0.6717 ± 0.0459 | 0.6243 ± 0.0408 | 0.8270 ± 0.1742 |\n",
      "| XGBoost      | {'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7}          | 0.6780 ± 0.0240 | 0.6356 ± 0.0206 | 0.7982 ± 0.0572 |\n",
      "| CatBoost     | {'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01}              | 0.6717 ± 0.0244 | 0.6329 ± 0.0231 | 0.7248 ± 0.0471 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.6226 ± 0.0294 | 0.6270 ± 0.0304 | 0.7956 ± 0.0307 |\n"
     ]
    }
   ],
   "source": [
    "for x in X_vals:\n",
    "    X = X_vals[x]\n",
    "    y = y_vals[x]\n",
    "    print(f\"Running models for {x} pipeline...\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Multi-Class Classification (no changes): PD vs DD vs HC\n",
    "    # -----------------------------------------------------------------------------\n",
    "    print(len(np.unique(y)))\n",
    "    if len(np.unique(y)) == 3:\n",
    "        print(\"Performing multi-class classification (PD vs DD vs HC) [Default Mode]...\")\n",
    "        results_default = run_cv(X, y, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "        overall_default = evaluate_cv(results_default, target_names=['HC', 'PD', 'DD'])\n",
    "    else:\n",
    "        print(\"Multi-class classification (PD vs DD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Multi-Class Classification with SMOTE: PD vs DD vs HC\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if len(np.unique(y)) == 3:\n",
    "        print(\"Performing multi-class classification (PD vs DD vs HC) with SMOTE...\")\n",
    "        results_smote = run_cv(X, y, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "        overall_smote = evaluate_cv(results_smote, target_names=['HC', 'PD', 'DD'])\n",
    "    else:\n",
    "        print(\"Multi-class classification with SMOTE is not possible with the current labels.\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Multi-Class Classification with Cost-Sensitive Learning (PD vs DD vs HC)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if len(np.unique(y)) == 3:\n",
    "        print(\"Performing multi-class classification (PD vs DD vs HC) with cost-sensitive learning...\")\n",
    "        classes = np.unique(y)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "        class_weights_multi = dict(zip(classes, weights))\n",
    "        print(\"Computed class weights:\", class_weights_multi)\n",
    "        \n",
    "        results_weighted = run_cv(\n",
    "            X, y,\n",
    "            models,\n",
    "            n_splits=5,\n",
    "            mode=\"weighted\",\n",
    "            class_weights=class_weights_multi,\n",
    "            tune_inner=True,\n",
    "            param_grids=param_grids\n",
    "        )\n",
    "        overall_weighted = evaluate_cv(results_weighted, target_names=['HC', 'PD', 'DD'])\n",
    "    else:\n",
    "        print(\"Multi-class classification with cost-sensitive learning is not possible with the current labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Binarios (PD vs DD) / (PD vs HC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing binary classification: PD vs DD [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7213\n",
      "Log Loss: 0.5173\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.6818\n",
      "Log Loss: 0.5375\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6722\n",
      "Log Loss: 0.6083\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7007\n",
      "Log Loss: 0.5125\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7638\n",
      "Log Loss: 0.4781\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7920\n",
      "Log Loss: 0.4870\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7806\n",
      "Log Loss: 0.4924\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7920\n",
      "Log Loss: 0.4845\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.7051\n",
      "F1 Score: 0.6722\n",
      "Log Loss: 0.5229\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6429\n",
      "Log Loss: 0.5642\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7051\n",
      "F1 Score: 0.6804\n",
      "Log Loss: 0.5938\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7020\n",
      "Log Loss: 0.5420\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7418\n",
      "Log Loss: 0.4843\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7469\n",
      "Log Loss: 0.4583\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7528\n",
      "Log Loss: 0.4971\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7773\n",
      "Log Loss: 0.4658\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6787\n",
      "Log Loss: 0.4985\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6960\n",
      "Log Loss: 0.5133\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7236\n",
      "Log Loss: 0.5956\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7448\n",
      "Log Loss: 0.4854\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7436\n",
      "[accuracy] Metric Standard Deviation: 0.0281\n",
      "[accuracy] Coefficient of Variation (CV): 3.78%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7156\n",
      "[f1_score] Metric Standard Deviation: 0.0355\n",
      "[f1_score] Coefficient of Variation (CV): 4.96%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5002\n",
      "[log_loss] Metric Standard Deviation: 0.0176\n",
      "[log_loss] Coefficient of Variation (CV): 3.52%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7333\n",
      "[accuracy] Metric Standard Deviation: 0.0424\n",
      "[accuracy] Coefficient of Variation (CV): 5.79%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7119\n",
      "[f1_score] Metric Standard Deviation: 0.0521\n",
      "[f1_score] Coefficient of Variation (CV): 7.32%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5121\n",
      "[log_loss] Metric Standard Deviation: 0.0371\n",
      "[log_loss] Coefficient of Variation (CV): 7.24%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.7436\n",
      "[accuracy] Metric Standard Deviation: 0.0292\n",
      "[accuracy] Coefficient of Variation (CV): 3.93%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7219\n",
      "[f1_score] Metric Standard Deviation: 0.0415\n",
      "[f1_score] Coefficient of Variation (CV): 5.74%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5574\n",
      "[log_loss] Metric Standard Deviation: 0.0515\n",
      "[log_loss] Coefficient of Variation (CV): 9.23%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7641\n",
      "[accuracy] Metric Standard Deviation: 0.0238\n",
      "[accuracy] Coefficient of Variation (CV): 3.11%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7434\n",
      "[f1_score] Metric Standard Deviation: 0.0376\n",
      "[f1_score] Coefficient of Variation (CV): 5.05%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.4981\n",
      "[log_loss] Metric Standard Deviation: 0.0265\n",
      "[log_loss] Coefficient of Variation (CV): 5.33%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[252  24]\n",
      " [ 76  38]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.77      0.91      0.83       276\n",
      "          DD       0.61      0.33      0.43       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.62      0.63       390\n",
      "weighted avg       0.72      0.74      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[243  33]\n",
      " [ 71  43]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.77      0.88      0.82       276\n",
      "          DD       0.57      0.38      0.45       114\n",
      "\n",
      "    accuracy                           0.73       390\n",
      "   macro avg       0.67      0.63      0.64       390\n",
      "weighted avg       0.71      0.73      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[246  30]\n",
      " [ 70  44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.78      0.89      0.83       276\n",
      "          DD       0.59      0.39      0.47       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.64      0.65       390\n",
      "weighted avg       0.72      0.74      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[250  26]\n",
      " [ 66  48]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.91      0.84       276\n",
      "          DD       0.65      0.42      0.51       114\n",
      "\n",
      "    accuracy                           0.76       390\n",
      "   macro avg       0.72      0.66      0.68       390\n",
      "weighted avg       0.75      0.76      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7436 ± 0.0281 | 0.7156 ± 0.0355 | 0.5002 ± 0.0176 |\n",
      "| XGBoost      | {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7}          | 0.7333 ± 0.0424 | 0.7119 ± 0.0521 | 0.5121 ± 0.0371 |\n",
      "| CatBoost     | {'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01}              | 0.7436 ± 0.0292 | 0.7219 ± 0.0415 | 0.5574 ± 0.0515 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.7641 ± 0.0238 | 0.7434 ± 0.0376 | 0.4981 ± 0.0265 |\n",
      "\n",
      "Performing binary classification: PD vs HC [Default Mode]...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.8976\n",
      "Log Loss: 0.2808\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.3049\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9134\n",
      "Log Loss: 0.3086\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8593\n",
      "Log Loss: 0.3415\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8643\n",
      "Log Loss: 0.3285\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8169\n",
      "F1 Score: 0.7967\n",
      "Log Loss: 0.3444\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8169\n",
      "F1 Score: 0.8222\n",
      "Log Loss: 0.3657\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8433\n",
      "Log Loss: 0.3142\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2708\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.5},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9025\n",
      "Log Loss: 0.2554\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2682\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9155\n",
      "Log Loss: 0.2394\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8391\n",
      "Log Loss: 0.3671\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8206\n",
      "Log Loss: 0.3506\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8557\n",
      "Log Loss: 0.2935\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8405\n",
      "Log Loss: 0.3468\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9287\n",
      "Log Loss: 0.2549\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8843\n",
      "Log Loss: 0.2241\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9266\n",
      "Log Loss: 0.1883\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9001\n",
      "Log Loss: 0.2091\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.8817\n",
      "[accuracy] Metric Standard Deviation: 0.0303\n",
      "[accuracy] Coefficient of Variation (CV): 3.44%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8796\n",
      "[f1_score] Metric Standard Deviation: 0.0308\n",
      "[f1_score] Coefficient of Variation (CV): 3.50%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3004\n",
      "[log_loss] Metric Standard Deviation: 0.0414\n",
      "[log_loss] Coefficient of Variation (CV): 13.78%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8648\n",
      "[accuracy] Metric Standard Deviation: 0.0303\n",
      "[accuracy] Coefficient of Variation (CV): 3.51%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8545\n",
      "[f1_score] Metric Standard Deviation: 0.0397\n",
      "[f1_score] Coefficient of Variation (CV): 4.64%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2959\n",
      "[log_loss] Metric Standard Deviation: 0.0494\n",
      "[log_loss] Coefficient of Variation (CV): 16.70%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0404\n",
      "[accuracy] Coefficient of Variation (CV): 4.60%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8773\n",
      "[f1_score] Metric Standard Deviation: 0.0383\n",
      "[f1_score] Coefficient of Variation (CV): 4.36%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2849\n",
      "[log_loss] Metric Standard Deviation: 0.0579\n",
      "[log_loss] Coefficient of Variation (CV): 20.34%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] CatBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0261\n",
      "[accuracy] Coefficient of Variation (CV): 2.97%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8717\n",
      "[f1_score] Metric Standard Deviation: 0.0305\n",
      "[f1_score] Coefficient of Variation (CV): 3.50%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2902\n",
      "[log_loss] Metric Standard Deviation: 0.0558\n",
      "[log_loss] Coefficient of Variation (CV): 19.23%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 55  24]\n",
      " [ 18 258]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.70      0.72        79\n",
      "          PD       0.91      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.82      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 45  34]\n",
      " [ 14 262]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.76      0.57      0.65        79\n",
      "          PD       0.89      0.95      0.92       276\n",
      "\n",
      "    accuracy                           0.86       355\n",
      "   macro avg       0.82      0.76      0.78       355\n",
      "weighted avg       0.86      0.86      0.86       355\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 54  25]\n",
      " [ 18 258]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.68      0.72        79\n",
      "          PD       0.91      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.81      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 49  30]\n",
      " [ 13 263]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.79      0.62      0.70        79\n",
      "          PD       0.90      0.95      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.84      0.79      0.81       355\n",
      "weighted avg       0.87      0.88      0.87       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}   | 0.8817 ± 0.0303 | 0.8796 ± 0.0308 | 0.3004 ± 0.0414 |\n",
      "| XGBoost      | {'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7}          | 0.8648 ± 0.0303 | 0.8545 ± 0.0397 | 0.2959 ± 0.0494 |\n",
      "| CatBoost     | {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}              | 0.8789 ± 0.0404 | 0.8773 ± 0.0383 | 0.2849 ± 0.0579 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20} | 0.8789 ± 0.0261 | 0.8717 ± 0.0305 | 0.2902 ± 0.0558 |\n",
      "\n",
      "Performing binary classification: PD vs DD with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6905\n",
      "Log Loss: 0.5438\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7202\n",
      "Log Loss: 0.5678\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.7007\n",
      "Log Loss: 0.6690\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7579\n",
      "Log Loss: 0.5436\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7623\n",
      "Log Loss: 0.4971\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7992\n",
      "Log Loss: 0.5474\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7719\n",
      "Log Loss: 0.5367\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8077\n",
      "F1 Score: 0.8109\n",
      "Log Loss: 0.5300\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7253\n",
      "Log Loss: 0.5244\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.6538\n",
      "F1 Score: 0.6667\n",
      "Log Loss: 0.5889\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.7041\n",
      "Log Loss: 0.5780\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6538\n",
      "F1 Score: 0.6516\n",
      "Log Loss: 0.5757\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7579\n",
      "Log Loss: 0.4538\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7579\n",
      "Log Loss: 0.5288\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7418\n",
      "Log Loss: 0.5318\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 31},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7548\n",
      "Log Loss: 0.4859\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7342\n",
      "Log Loss: 0.4925\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7658\n",
      "Log Loss: 0.5556\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7513\n",
      "Log Loss: 0.5926\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7507\n",
      "Log Loss: 0.5102\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7513\n",
      "[accuracy] Metric Standard Deviation: 0.0192\n",
      "[accuracy] Coefficient of Variation (CV): 2.55%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7340\n",
      "[f1_score] Metric Standard Deviation: 0.0258\n",
      "[f1_score] Coefficient of Variation (CV): 3.52%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5023\n",
      "[log_loss] Metric Standard Deviation: 0.0306\n",
      "[log_loss] Coefficient of Variation (CV): 6.09%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7410\n",
      "[accuracy] Metric Standard Deviation: 0.0482\n",
      "[accuracy] Coefficient of Variation (CV): 6.51%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7420\n",
      "[f1_score] Metric Standard Deviation: 0.0452\n",
      "[f1_score] Coefficient of Variation (CV): 6.10%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5577\n",
      "[log_loss] Metric Standard Deviation: 0.0201\n",
      "[log_loss] Coefficient of Variation (CV): 3.61%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0208\n",
      "[accuracy] Coefficient of Variation (CV): 2.78%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7340\n",
      "[f1_score] Metric Standard Deviation: 0.0276\n",
      "[f1_score] Coefficient of Variation (CV): 3.76%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5816\n",
      "[log_loss] Metric Standard Deviation: 0.0495\n",
      "[log_loss] Coefficient of Variation (CV): 8.52%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0510\n",
      "[accuracy] Coefficient of Variation (CV): 6.81%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7452\n",
      "[f1_score] Metric Standard Deviation: 0.0517\n",
      "[f1_score] Coefficient of Variation (CV): 6.94%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5291\n",
      "[log_loss] Metric Standard Deviation: 0.0303\n",
      "[log_loss] Coefficient of Variation (CV): 5.73%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[246  30]\n",
      " [ 67  47]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.89      0.84       276\n",
      "          DD       0.61      0.41      0.49       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.65      0.66       390\n",
      "weighted avg       0.73      0.75      0.74       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5}\n",
      "Confusion Matrix:\n",
      "[[222  54]\n",
      " [ 47  67]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.83      0.80      0.81       276\n",
      "          DD       0.55      0.59      0.57       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.70      0.69       390\n",
      "weighted avg       0.75      0.74      0.74       390\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[242  34]\n",
      " [ 64  50]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.79      0.88      0.83       276\n",
      "          DD       0.60      0.44      0.51       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.69      0.66      0.67       390\n",
      "weighted avg       0.73      0.75      0.74       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[231  45]\n",
      " [ 53  61]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.81      0.84      0.82       276\n",
      "          DD       0.58      0.54      0.55       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.69      0.69      0.69       390\n",
      "weighted avg       0.74      0.75      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7513 ± 0.0192 | 0.7340 ± 0.0258 | 0.5023 ± 0.0306 |\n",
      "| XGBoost      | {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5}          | 0.7410 ± 0.0482 | 0.7420 ± 0.0452 | 0.5577 ± 0.0201 |\n",
      "| CatBoost     | {'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01}              | 0.7487 ± 0.0208 | 0.7340 ± 0.0276 | 0.5816 ± 0.0495 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20} | 0.7487 ± 0.0510 | 0.7452 ± 0.0517 | 0.5291 ± 0.0303 |\n",
      "\n",
      "Performing binary classification: PD vs HC with SMOTE...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9003\n",
      "Log Loss: 0.3099\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8846\n",
      "Log Loss: 0.3140\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9003\n",
      "Log Loss: 0.3307\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8873\n",
      "Log Loss: 0.3204\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9057\n",
      "Log Loss: 0.3384\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8788\n",
      "Log Loss: 0.3641\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8310\n",
      "F1 Score: 0.8372\n",
      "Log Loss: 0.3886\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8803\n",
      "Log Loss: 0.3486\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9186\n",
      "Log Loss: 0.2849\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8643\n",
      "Log Loss: 0.4073\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8896\n",
      "Log Loss: 0.2842\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8769\n",
      "Log Loss: 0.2770\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.8310\n",
      "F1 Score: 0.8344\n",
      "Log Loss: 0.3230\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8643\n",
      "Log Loss: 0.2988\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8620\n",
      "Log Loss: 0.3215\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.3134\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.9577\n",
      "F1 Score: 0.9582\n",
      "Log Loss: 0.1981\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9155\n",
      "Log Loss: 0.2118\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9287\n",
      "Log Loss: 0.1705\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9304\n",
      "Log Loss: 0.1766\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.9014\n",
      "[accuracy] Metric Standard Deviation: 0.0408\n",
      "[accuracy] Coefficient of Variation (CV): 4.53%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.9034\n",
      "[f1_score] Metric Standard Deviation: 0.0400\n",
      "[f1_score] Coefficient of Variation (CV): 4.43%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2908\n",
      "[log_loss] Metric Standard Deviation: 0.0496\n",
      "[log_loss] Coefficient of Variation (CV): 17.04%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0211\n",
      "[accuracy] Coefficient of Variation (CV): 2.40%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8815\n",
      "[f1_score] Metric Standard Deviation: 0.0188\n",
      "[f1_score] Coefficient of Variation (CV): 2.13%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3192\n",
      "[log_loss] Metric Standard Deviation: 0.0660\n",
      "[log_loss] Coefficient of Variation (CV): 20.67%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.8817\n",
      "[accuracy] Metric Standard Deviation: 0.0340\n",
      "[accuracy] Coefficient of Variation (CV): 3.86%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8835\n",
      "[f1_score] Metric Standard Deviation: 0.0315\n",
      "[f1_score] Coefficient of Variation (CV): 3.57%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2991\n",
      "[log_loss] Metric Standard Deviation: 0.0725\n",
      "[log_loss] Coefficient of Variation (CV): 24.23%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] CatBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8873\n",
      "[accuracy] Metric Standard Deviation: 0.0218\n",
      "[accuracy] Coefficient of Variation (CV): 2.46%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8887\n",
      "[f1_score] Metric Standard Deviation: 0.0217\n",
      "[f1_score] Coefficient of Variation (CV): 2.45%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2872\n",
      "[log_loss] Metric Standard Deviation: 0.0598\n",
      "[log_loss] Coefficient of Variation (CV): 20.83%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "Confusion Matrix:\n",
      "[[ 67  12]\n",
      " [ 23 253]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.74      0.85      0.79        79\n",
      "          PD       0.95      0.92      0.94       276\n",
      "\n",
      "    accuracy                           0.90       355\n",
      "   macro avg       0.85      0.88      0.86       355\n",
      "weighted avg       0.91      0.90      0.90       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 63  16]\n",
      " [ 27 249]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.70      0.80      0.75        79\n",
      "          PD       0.94      0.90      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.82      0.85      0.83       355\n",
      "weighted avg       0.89      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 61  18]\n",
      " [ 24 252]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.72      0.77      0.74        79\n",
      "          PD       0.93      0.91      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.84      0.83       355\n",
      "weighted avg       0.89      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 63  16]\n",
      " [ 24 252]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.72      0.80      0.76        79\n",
      "          PD       0.94      0.91      0.93       276\n",
      "\n",
      "    accuracy                           0.89       355\n",
      "   macro avg       0.83      0.86      0.84       355\n",
      "weighted avg       0.89      0.89      0.89       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}     | 0.9014 ± 0.0408 | 0.9034 ± 0.0400 | 0.2908 ± 0.0496 |\n",
      "| XGBoost      | {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}          | 0.8789 ± 0.0211 | 0.8815 ± 0.0188 | 0.3192 ± 0.0660 |\n",
      "| CatBoost     | {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}              | 0.8817 ± 0.0340 | 0.8835 ± 0.0315 | 0.2991 ± 0.0725 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20} | 0.8873 ± 0.0218 | 0.8887 ± 0.0217 | 0.2872 ± 0.0598 |\n",
      "\n",
      "Performing binary classification: PD vs DD with cost-sensitive learning...\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.6920\n",
      "Log Loss: 0.5233\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.7095\n",
      "Log Loss: 0.5326\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.6722\n",
      "Log Loss: 0.6083\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7623\n",
      "Log Loss: 0.5614\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8205\n",
      "F1 Score: 0.8117\n",
      "Log Loss: 0.4855\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.7564\n",
      "F1 Score: 0.7642\n",
      "Log Loss: 0.5215\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7821\n",
      "F1 Score: 0.7806\n",
      "Log Loss: 0.4924\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7179\n",
      "F1 Score: 0.7290\n",
      "Log Loss: 0.5243\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.6923\n",
      "F1 Score: 0.6531\n",
      "Log Loss: 0.5290\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5},)\n",
      "Accuracy: 0.6795\n",
      "F1 Score: 0.6669\n",
      "Log Loss: 0.5804\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7051\n",
      "F1 Score: 0.6804\n",
      "Log Loss: 0.5938\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.01, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.6923\n",
      "F1 Score: 0.6959\n",
      "Log Loss: 0.5639\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 100},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7399\n",
      "Log Loss: 0.4829\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7920\n",
      "Log Loss: 0.4651\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7528\n",
      "Log Loss: 0.4971\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.7949\n",
      "F1 Score: 0.7949\n",
      "Log Loss: 0.4780\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.7308\n",
      "F1 Score: 0.6883\n",
      "Log Loss: 0.4720\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7569\n",
      "Log Loss: 0.4991\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.7236\n",
      "Log Loss: 0.5956\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.7692\n",
      "F1 Score: 0.7658\n",
      "Log Loss: 0.5105\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0434\n",
      "[accuracy] Coefficient of Variation (CV): 5.79%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7170\n",
      "[f1_score] Metric Standard Deviation: 0.0548\n",
      "[f1_score] Coefficient of Variation (CV): 7.64%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.4986\n",
      "[log_loss] Metric Standard Deviation: 0.0231\n",
      "[log_loss] Coefficient of Variation (CV): 4.63%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "RandomForest is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.7436\n",
      "[accuracy] Metric Standard Deviation: 0.0405\n",
      "[accuracy] Coefficient of Variation (CV): 5.45%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7379\n",
      "[f1_score] Metric Standard Deviation: 0.0443\n",
      "[f1_score] Coefficient of Variation (CV): 6.01%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5197\n",
      "[log_loss] Metric Standard Deviation: 0.0381\n",
      "[log_loss] Coefficient of Variation (CV): 7.33%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "XGBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.7436\n",
      "[accuracy] Metric Standard Deviation: 0.0292\n",
      "[accuracy] Coefficient of Variation (CV): 3.93%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7219\n",
      "[f1_score] Metric Standard Deviation: 0.0415\n",
      "[f1_score] Coefficient of Variation (CV): 5.74%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5574\n",
      "[log_loss] Metric Standard Deviation: 0.0515\n",
      "[log_loss] Coefficient of Variation (CV): 9.23%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "CatBoost is robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.7487\n",
      "[accuracy] Metric Standard Deviation: 0.0377\n",
      "[accuracy] Coefficient of Variation (CV): 5.03%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.7496\n",
      "[f1_score] Metric Standard Deviation: 0.0340\n",
      "[f1_score] Coefficient of Variation (CV): 4.54%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.5276\n",
      "[log_loss] Metric Standard Deviation: 0.0323\n",
      "[log_loss] Coefficient of Variation (CV): 6.13%\n",
      "[log_loss] Model is robust (CV < 10%)\n",
      "LightGBM is robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['PD', 'DD']\n",
      "Number of classes: 2\n",
      "Classes: ['PD', 'DD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[255  21]\n",
      " [ 77  37]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.77      0.92      0.84       276\n",
      "          DD       0.64      0.32      0.43       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.62      0.63       390\n",
      "weighted avg       0.73      0.75      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[231  45]\n",
      " [ 55  59]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.81      0.84      0.82       276\n",
      "          DD       0.57      0.52      0.54       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.68      0.68       390\n",
      "weighted avg       0.74      0.74      0.74       390\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[246  30]\n",
      " [ 70  44]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.78      0.89      0.83       276\n",
      "          DD       0.59      0.39      0.47       114\n",
      "\n",
      "    accuracy                           0.74       390\n",
      "   macro avg       0.69      0.64      0.65       390\n",
      "weighted avg       0.72      0.74      0.72       390\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[224  52]\n",
      " [ 46  68]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PD       0.83      0.81      0.82       276\n",
      "          DD       0.57      0.60      0.58       114\n",
      "\n",
      "    accuracy                           0.75       390\n",
      "   macro avg       0.70      0.70      0.70       390\n",
      "weighted avg       0.75      0.75      0.75       390\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.7487 ± 0.0434 | 0.7170 ± 0.0548 | 0.4986 ± 0.0231 |\n",
      "| XGBoost      | {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}          | 0.7436 ± 0.0405 | 0.7379 ± 0.0443 | 0.5197 ± 0.0381 |\n",
      "| CatBoost     | {'depth': 6, 'l2_leaf_reg': 3, 'learning_rate': 0.01}              | 0.7436 ± 0.0292 | 0.7219 ± 0.0415 | 0.5574 ± 0.0515 |\n",
      "| LightGBM     | {'learning_rate': 0.01, 'min_child_samples': 20, 'num_leaves': 20} | 0.7487 ± 0.0377 | 0.7496 ± 0.0340 | 0.5276 ± 0.0323 |\n",
      "\n",
      "Performing binary classification: PD vs HC with cost-sensitive learning...\n",
      "Computed class weights: {0: 2.2468354430379747, 1: 0.6431159420289855}\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8846\n",
      "Log Loss: 0.3128\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8495\n",
      "Log Loss: 0.3323\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9134\n",
      "Log Loss: 0.3086\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.9014\n",
      "F1 Score: 0.9003\n",
      "Log Loss: 0.2920\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8788\n",
      "Log Loss: 0.3519\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.7},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8563\n",
      "Log Loss: 0.3788\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 6, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8169\n",
      "F1 Score: 0.8222\n",
      "Log Loss: 0.3657\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 20, 'num_leaves': 20},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8689\n",
      "Log Loss: 0.3892\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': None, 'max_features': 'log2', 'n_estimators': 300},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2699\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8662\n",
      "Log Loss: 0.3201\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8732\n",
      "F1 Score: 0.8684\n",
      "Log Loss: 0.2682\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8915\n",
      "Log Loss: 0.2761\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100},)\n",
      "Accuracy: 0.8451\n",
      "F1 Score: 0.8467\n",
      "Log Loss: 0.3345\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8662\n",
      "Log Loss: 0.3274\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 1, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8557\n",
      "Log Loss: 0.2935\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.8592\n",
      "F1 Score: 0.8662\n",
      "Log Loss: 0.3283\n",
      "Tuning hyperparameters for RandomForest...\n",
      "Model results for RandomForest:\n",
      "Parameters: ({'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300},)\n",
      "Accuracy: 0.9718\n",
      "F1 Score: 0.9718\n",
      "Log Loss: 0.1892\n",
      "Tuning hyperparameters for XGBoost...\n",
      "Model results for XGBoost:\n",
      "Parameters: ({'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7},)\n",
      "Accuracy: 0.8873\n",
      "F1 Score: 0.8935\n",
      "Log Loss: 0.2638\n",
      "Tuning hyperparameters for CatBoost...\n",
      "Model results for CatBoost:\n",
      "Parameters: ({'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01},)\n",
      "Accuracy: 0.9296\n",
      "F1 Score: 0.9266\n",
      "Log Loss: 0.1883\n",
      "Tuning hyperparameters for LightGBM...\n",
      "Model results for LightGBM:\n",
      "Parameters: ({'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20},)\n",
      "Accuracy: 0.9155\n",
      "F1 Score: 0.9189\n",
      "Log Loss: 0.2130\n",
      "\n",
      "Robustness for model: RandomForest\n",
      "[accuracy] Metric Mean: 0.8901\n",
      "[accuracy] Metric Standard Deviation: 0.0431\n",
      "[accuracy] Coefficient of Variation (CV): 4.84%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8901\n",
      "[f1_score] Metric Standard Deviation: 0.0429\n",
      "[f1_score] Coefficient of Variation (CV): 4.82%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2917\n",
      "[log_loss] Metric Standard Deviation: 0.0581\n",
      "[log_loss] Coefficient of Variation (CV): 19.93%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] RandomForest is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: XGBoost\n",
      "[accuracy] Metric Mean: 0.8592\n",
      "[accuracy] Metric Standard Deviation: 0.0154\n",
      "[accuracy] Coefficient of Variation (CV): 1.80%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8664\n",
      "[f1_score] Metric Standard Deviation: 0.0150\n",
      "[f1_score] Coefficient of Variation (CV): 1.73%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.3245\n",
      "[log_loss] Metric Standard Deviation: 0.0367\n",
      "[log_loss] Coefficient of Variation (CV): 11.30%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] XGBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: CatBoost\n",
      "[accuracy] Metric Mean: 0.8789\n",
      "[accuracy] Metric Standard Deviation: 0.0404\n",
      "[accuracy] Coefficient of Variation (CV): 4.60%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8773\n",
      "[f1_score] Metric Standard Deviation: 0.0383\n",
      "[f1_score] Coefficient of Variation (CV): 4.36%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2849\n",
      "[log_loss] Metric Standard Deviation: 0.0579\n",
      "[log_loss] Coefficient of Variation (CV): 20.34%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] CatBoost is not robust across folds.\n",
      "\n",
      "\n",
      "Robustness for model: LightGBM\n",
      "[accuracy] Metric Mean: 0.8845\n",
      "[accuracy] Metric Standard Deviation: 0.0225\n",
      "[accuracy] Coefficient of Variation (CV): 2.55%\n",
      "[accuracy] Model is robust (CV < 10%)\n",
      "[f1_score] Metric Mean: 0.8891\n",
      "[f1_score] Metric Standard Deviation: 0.0198\n",
      "[f1_score] Coefficient of Variation (CV): 2.22%\n",
      "[f1_score] Model is robust (CV < 10%)\n",
      "[log_loss] Metric Mean: 0.2997\n",
      "[log_loss] Metric Standard Deviation: 0.0582\n",
      "[log_loss] Coefficient of Variation (CV): 19.43%\n",
      "[log_loss] Model is not robust (CV >= 10%)\n",
      "[ERROR] LightGBM is not robust across folds.\n",
      "\n",
      "Evaluating models with target names: ['HC', 'PD']\n",
      "Number of classes: 2\n",
      "Classes: ['HC', 'PD']\n",
      "Number of models: 4\n",
      "Models: ['RandomForest', 'XGBoost', 'CatBoost', 'LightGBM']\n",
      "Model: RandomForest\n",
      "Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Confusion Matrix:\n",
      "[[ 60  19]\n",
      " [ 20 256]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.76      0.75        79\n",
      "          PD       0.93      0.93      0.93       276\n",
      "\n",
      "    accuracy                           0.89       355\n",
      "   macro avg       0.84      0.84      0.84       355\n",
      "weighted avg       0.89      0.89      0.89       355\n",
      "\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}\n",
      "Confusion Matrix:\n",
      "[[ 70   9]\n",
      " [ 41 235]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.63      0.89      0.74        79\n",
      "          PD       0.96      0.85      0.90       276\n",
      "\n",
      "    accuracy                           0.86       355\n",
      "   macro avg       0.80      0.87      0.82       355\n",
      "weighted avg       0.89      0.86      0.87       355\n",
      "\n",
      "\n",
      "\n",
      "Model: CatBoost\n",
      "Parameters: {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
      "Confusion Matrix:\n",
      "[[ 54  25]\n",
      " [ 18 258]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.75      0.68      0.72        79\n",
      "          PD       0.91      0.93      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.81      0.82       355\n",
      "weighted avg       0.88      0.88      0.88       355\n",
      "\n",
      "\n",
      "\n",
      "Model: LightGBM\n",
      "Parameters: {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20}\n",
      "Confusion Matrix:\n",
      "[[ 70   9]\n",
      " [ 32 244]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          HC       0.69      0.89      0.77        79\n",
      "          PD       0.96      0.88      0.92       276\n",
      "\n",
      "    accuracy                           0.88       355\n",
      "   macro avg       0.83      0.89      0.85       355\n",
      "weighted avg       0.90      0.88      0.89       355\n",
      "\n",
      "\n",
      "\n",
      "Overall Metrics:\n",
      "| Model        | Params                                                             | Accuracy        | Weighted F1     | Log Loss        |\n",
      "|:-------------|:-------------------------------------------------------------------|:----------------|:----------------|:----------------|\n",
      "| RandomForest | {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}     | 0.8901 ± 0.0431 | 0.8901 ± 0.0429 | 0.2917 ± 0.0581 |\n",
      "| XGBoost      | {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7}          | 0.8592 ± 0.0154 | 0.8664 ± 0.0150 | 0.3245 ± 0.0367 |\n",
      "| CatBoost     | {'depth': 4, 'l2_leaf_reg': 3, 'learning_rate': 0.01}              | 0.8789 ± 0.0404 | 0.8773 ± 0.0383 | 0.2849 ± 0.0579 |\n",
      "| LightGBM     | {'learning_rate': 0.05, 'min_child_samples': 10, 'num_leaves': 20} | 0.8845 ± 0.0225 | 0.8891 ± 0.0198 | 0.2997 ± 0.0582 |\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Labels mapping for reference:\n",
    "# 0: HC\n",
    "# 1: PD\n",
    "# 2: DD\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD (Default Mode)\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD [Default Mode]...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0, DD (2) becomes 1\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "    \n",
    "    results_pd_dd_default = run_cv(X_pd_dd, y_pd_dd_binary, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_dd_default = evaluate_cv(results_pd_dd_default, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"\\nBinary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC (Default Mode)\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC [Default Mode]...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    results_pd_hc_default = run_cv(X_pd_hc, y_pd_hc, models, n_splits=5, mode=\"default\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_hc_default = evaluate_cv(results_pd_hc_default, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"\\nBinary classification (PD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD with SMOTE\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD with SMOTE...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0, DD (2) becomes 1\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "    \n",
    "    results_pd_dd_smote = run_cv(X_pd_dd, y_pd_dd_binary, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_dd_smote = evaluate_cv(results_pd_dd_smote, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC with SMOTE\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC with SMOTE...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    results_pd_hc_smote = run_cv(X_pd_hc, y_pd_hc, models, n_splits=5, mode=\"smote\", tune_inner=True, param_grids=param_grids)\n",
    "    overall_pd_hc_smote = evaluate_cv(results_pd_hc_smote, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs HC) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs DD with Cost-Sensitive Learning (Weighted Binary Mode)\n",
    "# -------------------------------\n",
    "if set([1, 2]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs DD with cost-sensitive learning...\")\n",
    "    mask_pd_dd = np.isin(y, [1, 2])\n",
    "    X_pd_dd = X[mask_pd_dd]\n",
    "    y_pd_dd = y[mask_pd_dd]\n",
    "    \n",
    "    # Adjust labels: PD (1) becomes 0 and DD (2) becomes 1.\n",
    "    y_pd_dd_binary = y_pd_dd - 1\n",
    "\n",
    "    # Compute balanced class weights from the original y_pd_dd\n",
    "    classes = np.unique(y_pd_dd)  # Typically [1, 2]\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_pd_dd)\n",
    "    # Create a dictionary mapping original classes to their weight.\n",
    "    class_weights_pd_dd = dict(zip(classes, weights))\n",
    "    # Remap keys to reflect the new labels: PD becomes 0, DD becomes 1.\n",
    "    class_weights_pd_dd = {k - 1: v for k, v in class_weights_pd_dd.items()}\n",
    "    print(\"Computed class weights for PD vs DD:\", class_weights_pd_dd)\n",
    "    \n",
    "    results_pd_dd_weighted = run_cv(\n",
    "        X_pd_dd,\n",
    "        y_pd_dd_binary,\n",
    "        models,\n",
    "        n_splits=5,\n",
    "        mode=\"weighted_binary\",\n",
    "        class_weights=class_weights_pd_dd,\n",
    "        tune_inner=True,\n",
    "        param_grids=param_grids\n",
    "    )\n",
    "    overall_pd_dd_weighted = evaluate_cv(results_pd_dd_weighted, target_names=['PD', 'DD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs DD) is not possible with the current labels.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Binary Classification: PD vs HC with Cost-Sensitive Learning (Weighted Binary Mode)\n",
    "# -------------------------------\n",
    "if set([1, 0]).issubset(set(y)):\n",
    "    print(\"\\nPerforming binary classification: PD vs HC with cost-sensitive learning...\")\n",
    "    mask_pd_hc = np.isin(y, [1, 0])\n",
    "    X_pd_hc = X[mask_pd_hc]\n",
    "    y_pd_hc = y[mask_pd_hc]\n",
    "    \n",
    "    # Compute balanced class weights from the current training subset.\n",
    "    # Here, we assume that in PD vs HC, class 0 = HC and class 1 = PD.\n",
    "    classes = np.unique(y_pd_hc)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_pd_hc)\n",
    "    class_weights_pd_hc = dict(zip(classes, weights))\n",
    "    print(\"Computed class weights:\", class_weights_pd_hc)\n",
    "    \n",
    "    results_pd_hc_weighted = run_cv(\n",
    "        X_pd_hc,\n",
    "        y_pd_hc,\n",
    "        models,\n",
    "        n_splits=5,\n",
    "        mode=\"weighted_binary\",\n",
    "        class_weights=class_weights_pd_hc,\n",
    "        tune_inner=True,\n",
    "        param_grids=param_grids\n",
    "    )\n",
    "    overall_pd_hc_weighted = evaluate_cv(results_pd_hc_weighted, target_names=['HC', 'PD'])\n",
    "else:\n",
    "    print(\"Binary classification (PD vs HC) is not possible with the current labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
